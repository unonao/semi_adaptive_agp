%\vspace{-2mm}
\section{Introduction} \label{sec:intro}
Recently, significant research effort has been devoted to compute {\em node proximty queries} such as Personalized PageRank~\cite{page1999pagerank,jung2017bepi,Wang2017FORA,wei2018topppr}, heat kernel PageRank~\cite{chung2007HKPR,yang2019TEA} and the Katz score~\cite{katz1953Katz}. %Node proximity queries find numerous applications in the area of graph mining, such as link prediction in social networks~\cite{backstrom2011supervised, Liben2003link}, personalized graph search techniques~\cite{jeh2003scaling,ivan2011cell}, fraud detection~\cite{andersen2008robust,Spirin2011spam}, and collaborative filtering in recommender networks~\cite{gupta2013wtf,liu2017related}. 
Given a node $s$ in an undirected graph $G=(V,E)$ with $|V|=n$ nodes and $|E|=m$ edges, a node proximity query  returns an $n$-dimensional vector $\vec{\pi}$ such that $\vec{\pi}(v)$ represents the importance of node $v$ with respect to $s$.  
% Some of the most widely used proximity measures include transition probabilities, PageRank and Personalized PageRank~\cite{page1999pagerank}, the Katz score~\cite{katz1953Katz}, and heat kernel PageRank~\cite{chung2007HKPR}. 
For example, a widely used proximity measure is the $ L $-th transition probability vector. It captures the $ L $-hop neighbors' information by computing the probability that a $ L $-step random walk from a given source node $s$ reaches each node in the graph. 
The vector form is given by  $	\vec{\pi}= \left(\mathbf{A} \mathbf{D}^{-1} \right)^L \cdot \vec{e}_s,$
% \begin{equation}\label{eqn:transition_definition}
% 	\begin{aligned}
% 		\vec{\pi}= \left(\mathbf{A} \mathbf{D}^{-1} \right)^L \cdot \vec{e}_s,
% 	\end{aligned}
% \end{equation}
where $\A $ is the adjacency matrix, $\D$ is the diagonal degree matrix with $\D(i,i) = \sum_{j \in V} \A(i,j)$, and $\vec{e}_s$ is the one-hot vector with $\vec{e}_s(s)=1$ and $\vec{e}_s(v)=0, v\neq s$. 
Node proximity queries find numerous applications in the area of graph mining, such as link prediction in social networks~\cite{backstrom2011supervised}, personalized graph search techniques~\cite{jeh2003scaling}, fraud detection~\cite{andersen2008robust}, and collaborative filtering in recommender networks~\cite{gupta2013wtf}. 
% Recently, there is a trends to use node proximities to model and scale up the Graph Neural Networks (GNNs). In particular, 
 
 
 
 
%In particular, a recent trend in Graph Neural Networks (GNNs)  research~\cite{wu2019SGC,klicpera2019GDC,Klicpera2018APPNP} employs node proximity query to build scalable GNN models. 
In particular, a recent trend in Graph Neural Network (GNN)  researches~\cite{wu2019SGC,klicpera2019GDC,Klicpera2018APPNP} is to employ node proximity queries to build scalable GNN models.
A typical example is SGC~\cite{wu2019SGC}, which simplifies the original Graph Convolutional Network (GCN)~\cite{kipf2016GCN} with a linear propagation process. More precisely, given a self-looped graph and an  $n \times d$ feature matrix $\mathbf{X}$, 
%SGC takes the multiplication of $L$-th power of the normalized adjacency matrix $\mathbf{D}^{-\frac{1}{2}}  \mathbf{A} \mathbf{D}^{-\frac{1}{2}} $ and feature matrix  $\mathbf{X}$ 
SGC takes the multiplication of the $L$-th normalized transition probability matrix $\P=\left(\mathbf{D}^{-\frac{1}{2}}  \mathbf{A}  \mathbf{D}^{-\frac{1}{2}} \right)^L$ and the feature matrix $\mathbf{X}$
to form the representation matrix %$\mathbf{Z}\hspace{-0.5mm}= \hspace{-0.5mm}\left(\mathbf{D}^{-\frac{1}{2}}  \mathbf{A}  \mathbf{D}^{-\frac{1}{2}} \right)^L \hspace{-1mm}\cdot \hspace{-0.5mm}\X$. 
\begin{equation}
\begin{aligned}\label{eqn:SGC}
\mathbf{Z}=\P \cdot \X= \left(\mathbf{D}^{-\frac{1}{2}} \cdot \mathbf{A} \cdot \mathbf{D}^{-\frac{1}{2}} \right)^L \cdot \vec{X}.
\end{aligned}	
\end{equation}
If we treat each column of the feature matrix $\X$ as a graph signal vector $\bm{x}$, then the representation matrix $\mathbf{Z}$ can be derived by the augment of $d$ vectors $\bm{\pi}\hspace{-0.5mm}= \hspace{-0.5mm}\left(\mathbf{D}^{-\frac{1}{2}} \mathbf{A}  \mathbf{D}^{-\frac{1}{2}} \right)^L \hspace{-1mm}\cdot \bm{x}$. SGC feeds $\mathbf{Z}$ into a logistic regression or a standard neural network for downstream machine learning tasks such as node classification and link prediction. 
%Similarly, APPNP~\cite{Klicpera2018APPNP}, GDC~\cite{klicpera2019GDC} and GBP~\cite{chen2020GBP} utilize Personalized PageRank and Heat Kernel PageRank to capture multi-hop neighborhood information. 
The feature propagation matrix $\P$ can be easily generalized to other node proximity models, such as PPR used in APPNP~\cite{Klicpera2018APPNP}, PPRGo~\cite{bojchevski2020scaling} and GBP~\cite{chen2020GBP}, and HKPR used in~\cite{klicpera2019GDC}. 
Compared to the original GCN~\cite{kipf2016GCN} which uses a full-batch training process and stores the representation of each node in the GPU memory, these proximity-based GNNs decouple prediction and propagation and thus allows mini-batch training to improve the scalability of the models. 
Note that even though the ideas to employ PPR and HKPR models in the feature propagation process are borrowed from APPNP, PPRGo, GBP and GDC, the original papers of APPNP, PPRGo and GDC use extra complex structures to propagate node features. With a slight abuse of notation, we use APPNP and GDC to denote the linear propagation process by substituting $\P$ in equation~\eqref{eqn:SGC} to PPR and HKPR models, respectively. 




\header{\bf Graph Propagation.} 
%\header{\bf Problem Definitions. } 
  To model various proximity measures and GNN propagation formulas, we consider the following unified {\em graph propagation equation}:   
\vspace{-2mm}
\begin{equation}\label{eqn:pi_gen}
\vspace{-1mm}
	\begin{aligned}
		\vec{\pi}=\sum_{i=0}^\infty w_i \cdot \left(\mathbf{D}^{-a} \mathbf{A}  \mathbf{D}^{-b} \right)^i \cdot \vec{x}, 
	\end{aligned}
\end{equation}
where $\mathbf{A}$  denotes the adjacency matrix, $\mathbf{D}$ denotes the diagonal degree matrix, $a$ and $b$ are the Laplacian parameters that take values in $[0,1]$, the sequence of $w_i$ for $i=0,1,2,...$ is the weight sequence and $\bm{x}$ is an $n$ dimensional vector. Following the convention of Graph Convolution Networks~\cite{kipf2016GCN}, we refer to $\mathbf{D}^{-a}\mathbf{A}\mathbf{D}^{-b}$ as the {\em propagation matrix}, and $\vec{x}$ as the {\em graph signal vector}. 


A key feature of the graph propagation equation~\eqref{eqn:pi_gen} is that we can manipulate parameters $a$, $b$, $w_i$ and $\vec{x}$ to obtain different proximity measures. For example, if we set  $a=0, b=1, w_L=1,   w_i=0$ for $i=0,\ldots, L-1$, and $\vec{x}=\vec{e}_s$, then $\vec{\pi}$ becomes the $L$-th transition probability vector from node $s$. Table~\ref{tbl:propagation} summarizes the proximity measures and GNN models that can be expressed by Equation~\eqref{eqn:pi_gen}.  




\header{\bf Approximate Graph Propagation (AGP). } 
In general, it is computational infeasible to compute Equation~\eqref{eqn:pi_gen}
exactly as the summation goes to infinity. Following~\cite{bressan2018sublinear,wang2020RBS},   we will consider an approximate  version of the graph propagation equation~\eqref{eqn:pi_gen}:
\begin{definition}[Approximate propagation with relative error]\label{def:pro-relative}
	%\caption{Node Incode, Edge Saving and Edge Expense in forwardPush}
	Let $\vec{\pi}$ be the graph propagation vector defined in Equation~\eqref{eqn:pi_gen}.
% 	graph $G=(V,E)$, a vector $\vec{x}$ over $n$ nodes, two laplacian parameters $a$ and $b$,  a weight sequence $w_i$ for $i\ge 0$, and 
	Given an error threshold $\delta$, an approximate propagation algorithm has to return an estimation vector $\hat{\vec{\pi}}$, such that  for any $v \in V$, with  $|\vec{\pi}(v)|>\delta$, we have 
	\vspace{-2mm}
	\begin{align}\nonumber
	\vspace{-2mm}
	\left|\vec{\pi}(v)-\hat{\vec{\pi}}(v)\right| \leq \frac{1}{10} \cdot \vec{\pi}(v)
	\end{align}
	 with at least a constant probability (e.g. $99\%$).
\end{definition}
%The above relative error guarantee follows from previous work on PageRank and heat kernel PageRank~\cite{bressan2018sublinear}. 
We note that some previous works~\cite{yang2019TEA,Wang2017FORA} consider the guarantee $\left|\vec{\pi}(v)-\hat{\vec{\pi}}(v)\right| \leq \varepsilon_r \cdot \vec{\pi}(v)$ with probability at least $1-p_f$, where $\varepsilon_r$ is the relative error parameter and $p_f$ is the fail probability.  However, $\varepsilon_r$ and $p_f$ are set to be constant in these works. For sake of simplicity and readability, we set $\varepsilon_r = 1/10$ and $p_f = 1\%$ and introduce only one error parameter $\delta$, following the setting in~\cite{bressan2018sublinear,wang2020RBS}.

% For simplicity, we first consider the undirected graph. We will show how to extend the graph propagation equation~\eqref{eqn:pi_gen} on directed graphs in Section~\ref{sec:other}.


% As real-world graphs, such as social networks and citation networks, have grown in popularity at an extraordinary pace, efficient computation of the proximity queries has also been extensively studied over the last few years. Exact computations of various proximity queries often rely on iterative methods, which generally takes at least $O((m+n)L)$ time, where $n$ is the number of nodes, $m$ is the number of edges, and $L$ is the number of iterations. Such high time complexity limits the algorithm's scalability and makes them infeasible to support real-time proximity queries on large graphs.  Fortunately, many graph mining applications can tolerate a small error of the proximity vector. Therefore, many works focus on the approximate computation of various proximity vectors~\cite{lofgren2014fast,lofgren2015personalized,LofgrenBG15,Teng2004Nibble,yang2019TEA,chung2018computing}.



\header{\bf Motivations.} 
Existing works on proximity computation and GNN feature propagation are on a case-by-case basis, with each paper focusing on a particular proximity measure. For example, despite the similarity between Personalized PageRank and heat kernel PageRank, the two proximity measures admit two completely different sets of algorithms (see~\cite{wei2018topppr,Wang2016HubPPR,jung2017bepi,Shin2015BEAR,coskun2016efficient} for Personalized PageRank and~\cite{yang2019TEA,chung2007HKPR,chung2018computing,kloster2014heat} for heat kernel PageRank). 
% Moreover, 
% existing feature propagation methods in GNN do not leverage the approximate algorithms for proximity computation, limiting their scalability.  
Therefore, a natural question is 
\begin{quote}
	{\em Is there a universal algorithm that computes the approximate graph propagation with near optimal cost? }
\end{quote}


 
 
%  On the other hand, we observe a strong resemblance between Equation~\eqref{eqn:transition_definition} and ~\eqref{eqn:SGC}, which suggests that it is possible to unify transition probability and the feature propagation of SGC. Such unity also implies that we may also be able to use an approximate algorithm for transition probability to perform approximate propagation from each column of the feature matrix $\mathbf{X}$ in SGC. Therefore, a natural question is whether we can extend this unity to other proximity measures. In this paper, we exploit the possibility of providing a universal way to characterize various proximity measures and enable efficient approximate algorithms for these proximity measures.
%  Furthermore, the hope is that by unifying proximity computation and GNN feature propagation, we can improve the existing GNN models' scalability.

% \begin{quote}
% 	{\em Is there a universal algorithm to approximate Equation~\eqref{eqn:pi_gen}?}
% %	, like the $O(m \cdot \log \frac{1}{\lmd})$-bound of $\powitr$?}
% \end{quote}

% However, these methods still suffer from scalability issues on graphs with billions of edges as they compute the proximity exactly with iterative methods. 




% directly apply a proximity matrix to the feature matrix

% Graph Neural Networks (GNNs) has drawn increasing attention due to its wide range of applications such as social analysis~\cite{qiu2018network,li2019encoding},
% biology~\cite{DBLP:conf/nips/FoutBSB17,shang2019gamenet}, and recommender systems~\cite{ying2018graph}. 

% Recall that in the settings of GNN, each node $v$ is also associated with a numeric feature vector of dimension $d$. The $n$ feature vectors form an $n \times d$ matrix $\mathbf{X}$. The goal of GNN is to obtain an $n\times d'$ representation matrix $\mathbf{Z}$, which encodes both the graph structural information and the feature matrix $\mathbf{X}$. We then feed $\mathbf{Z}$ into a neural network for downstream machine learning tasks such as node classification and link prediction. 


% The seminal paper~\cite{kipf2016GCN} proposes  Graph Convolutional Network (GCN), which adopts a message-passing approach and gathers information from the neighbors of each node from the previous layer to form the new representation. However, the vanilla GCN uses a full-batch training process and stores the representation of each node in the GPU memory,  which leads to limited scalability.  







% To scale GNN on large graphs,  a recent trend in GNN research~\cite{wu2019simplifying,klicpera2019GDC,Klicpera2018APPNP} directly apply a proximity matrix to the feature matrix $\mathbf{X}$ to obtain the propagation matrix $\mathbf{Z}$, and then feed $\mathbf{Z}$ into logistic regression or standard neural network for predictions. 

% For example, SGC~\cite{wu2019simplifying}  taking the multiplication of $L$-th power of the normalized adjacency matrix $\mathbf{A}$ and feature matrix  $\mathbf{X}$ to form the final  presentation
% \begin{equation}
% \begin{aligned}\label{eqn:SGC}
% \mathbf{Z}= \left(\mathbf{D}^{-\frac{1}{2}} \cdot \mathbf{A} \cdot \mathbf{D}^{-\frac{1}{2}} \right)^L \cdot \vec{X}.
% \end{aligned}	
% \end{equation}
% As we will discuss in Section~\ref{sec:pre}, other methods utilize Personalized PageRank and heat kernel PageRank to capture multi-hop neighborhood information. However, these methods still suffer from scalability issues on graphs with billions of edges as they compute the proximity exactly with iterative methods. 

 
 


% %\subsection{Problem Definition and Contributions} \label{subsec:applications}






% % Since $\|\vec{x}\|_1 =1$, we have $\|\vec{x}^+\|_1 \le 1$ and  $\|\vec{x}^-\|_1 \le 1$, which means the decomposition will not affect the asymptotic time complexity. 





\begin{table*}[t]
	%\vspace{-5mm}
	\centering
	\renewcommand{\arraystretch}{1.5}
	\tblcapup
	\caption{Typical graph propagation equations.}
	\vspace{-4mm}
	\tblcapdown
	\begin{small}
		\begin{tabular}{|c|c|c|c|c|c|c|} \hline

					~&{\bf Algorithm} & {\bf $\boldsymbol{a}$}& {\bf $\boldsymbol{b}$} & {\bf $\boldsymbol{w_i}$} & {\bf $\boldsymbol{\vec{x}}$} & {\bf Propagation equation} \\ \cline{1-7}
			\multirow{6}{*}{Proximity} &	{$L$-hop transition probability} & $0$ & $1$ & $w_i=0 (i \neq L), w_L=1$ & one-hot vector $\vec{e}_s$ & $\vec{\pi}=\left(\mathbf{A} \mathbf{D}^{-1} \right)^L \cdot \vec{e}_s $\\ \cline{2-7}
			~&{PageRank}~\cite{page1999pagerank} & $0$ & $1$& $\alpha \left( 1-\alpha \right)^i $ & $\frac{1}{n}\cdot \vec{1}$ & $\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \hspace{-1mm}\cdot\hspace{-0.5mm} \left(\mathbf{A} \mathbf{D}^{-1} \right)^i \hspace{-1mm}\cdot \hspace{-0.5mm} \frac{\vec{1}}{n}$ \\ \cline{2-7}
			~&{Personalized PageRank}~\cite{page1999pagerank} &$0$ & $1$ & $\alpha \left( 1-\alpha \right)^i $ & teleport probability distribution $x$ & $\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A} \mathbf{D}^{-1} \right)^i \cdot \bm{x} $ \\ \cline{2-7}
			~&{single-target PPR}~\cite{lofgren2013personalized} & $1$ & $0$ & $\alpha \left( 1-\alpha \right)^i $ & one-hot vector $\vec{e}_v$ & $\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-1} \mathbf{A} \right)^i \cdot \vec{e}_v $\\ \cline{2-7}
			~&{heat kernel PageRank}~\cite{chung2007HKPR}  &$0$ & $1$ & $e^{-t} \cdot \frac{t^i}{i!} $ & one-hot vector $\vec{e}_s$ & $\vec{\pi}=\sum_{i=0}^\infty e^{-t} \cdot \frac{t^i}{i!}\cdot \left(\mathbf{A} \mathbf{D}^{-1} \right)^i \cdot \vec{e}_s $\\ \cline{2-7}
			~&{Katz}~\cite{katz1953Katz} & $0$ & $0$ & $\beta^i$ & one-hot vector $\vec{e}_s$ & $\vec{\pi}=\sum_{i=0}^\infty \beta^i  \mathbf{A}^i \cdot \vec{e}_s$\\ \hline
			
			\multirow{3}{*}{GNN} & SGC~\cite{wu2019SGC} & $\frac{1}{2}$ & $\frac{1}{2}$ & $w_i=0 (i \neq L), w_L=1$ & the graph signal $\bm{x}$ & $\vec{\pi}=\left(\mathbf{D}^{-\frac{1}{2}} \mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^L \cdot \vec{x} $\\ \cline{2-7}
			~& APPNP~\cite{Klicpera2018APPNP}  &  $\frac{1}{2}$ & $\frac{1}{2}$ & $\alpha \left( 1-\alpha \right)^i$ & the graph signal $\bm{x}$ & $\vec{\pi}=\sum_{i=0}^L \alpha \left( 1-\alpha \right)^i \hspace{-1mm}\cdot \hspace{-0.5mm} \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^i \hspace{-1.5mm}\cdot \hspace{-0.5mm} \vec{x} $ \\ \cline{2-7}
			~& GDC~\cite{klicpera2019GDC}  &  $\frac{1}{2}$ & $\frac{1}{2}$ & $e^{-t} \cdot \frac{t^i}{i!} $  & the graph signal $\bm{x}$ & $\vec{\pi}=\sum_{i=0}^L e^{-t} \cdot \frac{t^i}{i!}\hspace{-1mm}\cdot \hspace{-0.5mm} \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A}\mathbf{D}^{-\frac{1}{2}} \right)^i \hspace{-1.5mm}\cdot \hspace{-0.5mm} \vec{x} $ \\ \hline
		\end{tabular}
	\end{small}
	\label{tbl:propagation}
	%\tbldown
	\vspace{-3mm}
\end{table*}







 




\header{\bf Contributions.} In this paper, we present AGP, a UNIFIED randomized algorithm that computes Equation~\eqref{eqn:pi_gen} with almost optimal computation time and theoretical error guarantee. AGP naturally generalizes to  various proximity measures, including transition probabilities, PageRank and Personalized PageRank, heat kernel PageRank, and Katz. We conduct an extensive experimental study to demonstrate the effectiveness of AGP on real-world graphs. We show that AGP outperforms the state-of-the-art methods for local graph clustering with heat kernel PageRank.  We  also show that AGP can scale various GNN models (including SGC~\cite{wu2019SGC}, APPNP~\cite{Klicpera2018APPNP}, and GDC~\cite{klicpera2019GDC}) up on the billion-edge graph Papers100M, which is the largest publicly available  GNN dataset.



%What's more, this provides a generalized view to capture the core of propagation process, and understand the  









%Most propagation techniques restrict the weighted sequence as $\sum_{i=0}^{\infty}w_i=1$, such as PageRank~\cite{page1999pagerank}, PPR~\cite{page1999pagerank}, HKPR~\cite{chung2007HKPR} and GNN methods. We lists some typical propagation equations in Table~\ref{tbl:propagation}. 

%Note that in equation~\eqref{eqn:pi}, the sum of matrix $\mathbf{D}$'s exponent equals $-1$. If we lift this restriction and allow $\sum_{i=0}^{\infty}w_i \neq 1$ meanwhile, we can cover Katz index~\cite{katz1953Katz} and HITS~\cite{kleinberg1999HITS} by the generalized propagation equation given below. 


%Specifically, when we set $a=b=0$, equation~\eqref{eqn:pi_gen} becomes $\pi=\sum_{i=0}^\infty w_i \cdot \left(\mathbf{A}^\top \right)^i \cdot \vec{x}$, which equals to the definition equation of Katz. Let the signal vector $x$ be the one-hot vector $\vec{e}_s$. By equation~\eqref{eqn:pi_gen}, we can derive the vector of Katz index between node $s$ and any node in the graph. Furthermore, if we only focus on undirected graph and substitute the adjacency matrix $\mathbf{A}$ as $\mathbf{A}^2$ in equation~\eqref{eqn:pi_gen}, the HITS scores for either hubs or authorities can be obtained by equation~\eqref{eqn:pi_gen} that $\pi=\sum_{i=0}^\infty w_i \cdot \left(\mathbf{A}^2\right)^i \cdot \vec{x}$.  The detailed analysis towards Katz and HITS are illustrated in Section~\ref{sec:other}. In the other parts of this paper, we only focus on equation~\eqref{eqn:pi} and restrict $\sum_{i=0}^{\infty}w_i=1$. 


%\begin{equation}\label{eqn:pi}
%	\begin{aligned}
%		\pi=\sum_{i=0}^\infty w_i \cdot \left(\mathbf{D}^{-(1-r)}\cdot \mathbf{A}^\top \cdot \mathbf{D}^{-r} \right)^i \cdot \vec{x}, 
%	\end{aligned}
%\end{equation}

 

% 			\multicolumn{2}{|c|}{\bf Algorithm} & {\bf $\boldsymbol{a}$}& {\bf $\boldsymbol{b}$} & {\bf $\boldsymbol{w_i}$} & {\bf $\boldsymbol{\vec{x}}$} & {\bf Propagation equation} \\ \hline
% 			\multicolumn{2}{|c|}{$k$-hop transition probability} & $a=0$ & $b=1$ & $w_i=0 (i \neq k), w_k=1$ & one-hot vector $\vec{e}_s$ & $\pi=\left(\mathbf{A} \cdot \mathbf{D}^{-1} \right)^k \cdot \vec{e}_s $\\ \hline
% 			\multicolumn{2}{|c|}{PageRank} & $a=0$ & $b=1$& $\alpha \left( 1-\alpha \right)^i $ & $\frac{1}{n}\cdot \vec{1}$ & $\pi=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A} \cdot \mathbf{D}^{-1} \right)^i \cdot \left(\frac{1}{n}\cdot \vec{1} \right)$ \\ \hline
% 			\multicolumn{2}{|c|}{single-source PPR} &$a=0$ & $b=1$ & $\alpha \left( 1-\alpha \right)^i $ & one-hot vector $\vec{e}_s$ & $\pi=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A}  \cdot \mathbf{D}^{-1} \right)^i \cdot \vec{e}_s $ \\ \hline
% 			\multicolumn{2}{|c|}{single-target PPR} & $a=1$ & $b=0$ & $\alpha \left( 1-\alpha \right)^i $ & one-hot vector $\vec{e}_t$ & $\pi=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-1} \cdot \mathbf{A} \right)^i \cdot \vec{e}_t $\\ \hline
% 			\multicolumn{2}{|c|}{HKPR}  &$a=0$ & $b=1$ & $e^{-t} \cdot \frac{t^i}{i!} $ & one-hot vector $\vec{e}_s$ & $\pi=\sum_{i=0}^\infty e^{-t} \cdot \frac{t^i}{i!}\cdot \left(\mathbf{A} \cdot \mathbf{D}^{-1} \right)^i \cdot \vec{e}_s $\\ \hline
% 			\multicolumn{2}{|c|}{Katz} & $a=0$ & $b=0$ & $\beta^i$ & one-hot vector $\vec{e}_s$ & $\pi=\sum_{i=0}^\infty \beta^i \cdot \left(\mathbf{A} \right)^i \cdot \vec{x}$\\ \hline
% 			\multicolumn{2}{|c|}{HITS} & $a=0$ & $b=0$ & $w_i=\frac{1}{\mu_{j}}(i=2j)$, or $w_i=0$ & $\frac{1}{n}\cdot \vec{1}$ & $\pi=\sum_{j=0}^\infty \frac{1}{\mu_j}\cdot \left(\mathbf{A}\right)^{2j} \cdot \vec{1}$ \\ \hline	










% \begin{comment}
% \pprfull(PPR) is a fundamental metric to evaluate a node's relative significance with respect to a given source node.
% Its definition is given by ~\cite{page1999pagerank} in the form of matrix vector multiplication. 
% \begin{align}
% \overrightarrow{\pi_s}=(1-\alpha)\cdot \overrightarrow{\pi_s}\mathbf{P}+\alpha \cdot \overrightarrow{e_s}
% \end{align}
% Here $\pi_s$ denotes the PPR vector corresponding to a source node $s$ and $\mathbf{P}$ represents the transition probability determined by the graph structure.
% The initial PPR vector is a one-hot vector with 1 assigned to source node $s$. 
% In each transition step, the current node may return to the initial state with $\alpha$ probability or transform to its neighborhood equally.
% Such definition can be equivalently expressed as a random walk process. 
% Start from node $s$ and step into its neighbor nodes uniformly randomly with $1-\alpha$ probability.
% Meanwhile, the walk may choose back to $s$ in each step with the left $\alpha$ probability.

% Limited to the multiplication between transition matrix $\mathbf{P}$ and state vector $\overrightarrow{\pi_s}$, the time complexity is $\Omega(m)$, where $m$ is the number of edges. 
% Worse than that, even if we only focus on single-pair PPR results, PPR between two specific given nodes, it has to conduct $\Omega(m)$ times multiplication operations and obtains single-source PPR results unavoidably. 
% This renders single-pair PPR's efficient computation when $m$ is large.

% For this reason, massive works focus on the approximation calculation when brings in a trade-off between time and accuracy. 
% When we consider $c$ relative error towards $\pi(s,t)$ with $\pi(s,t)>\delta$, $\Theta(\frac{1}{\sqrt{\delta}})$ gives a lower bound time complexity to derive qualified single-pair PPR results if we regard $c$ as a constant, which is proved in ~\cite{Lofgren2014FASTPPR}.
% However, to our knowledge, none existing work can achieve this complexity. 
% If we divide mainstream algorithms into three categories, matrix-based, Monte-Carlo-based and push-based, then most works can be put into the three classes or their mixtures. 
% Matrix-based methods aim at the definition equation's linear rewriting and confront to $\Omega(m)$ complexity as the definition does.  
% Monte-Carlo based approaches~\cite{fogaras2005towards} stay in $O(\frac{1}{\delta})$ time complexity due to Chernoff bound's format. 
% Forward Push~\cite{AndersenCL06} can only obtain exact PPR scores with a high cost or end algorithm in the midway without guarantee of accuracy. 
% If two given node is selected randomly, then Backward Push~\cite{AndersenBCHMT07} is able to offer an error bound with $O(\frac{d}{\delta})$ time complexity, where $d$ denotes the average degree of the given graph.  
% Compared with straightforward methods, a mixture of two methods performs better. 
% Combined with Monte-Carlo and Backward Push, FAST-PPR~\cite{Lofgren2014FASTPPR}  can achieve $O(\frac{d}{\sqrt{\delta}})$ complexity when two pair nodes chosen randomly either. 
% Bidirectional-PPR~\cite{lofgren2015personalized} reaches the same complexity as FAST-PPR with Forward Push and Backward Push's integration.
% \end{comment}



%\header{\bf Motivations.}

% Graph, as an abstract structure to model the relationships between objects, can be found almost everywhere in our life, such as social networks, citation networks, geographic networks and so on. In recent years, graph researches emerge and grow into a popular trend, which are applied to multiple practical problems. Reviewing these works, we can roughly classify them into two categories: graph analysis tasks and graph learning tasks. Graph analysis aims to efficiently compute some specific metrics based on the graph topology. These metrics are expected to effectively reflect the graph structure information, such as node proximity or similarity. For example, the $\ell$-hop transition probability uses random walks at $\ell$ steps to capture the information of $\ell$-hop neighbors. 
% %However, by the small-world theory~\cite{milgram1967small}, graphs abstracted from real network always has small diameters. The multi-hop transition probabilities of nodes on real netwroks can easily achieve iden. 
% PageRank and Personalized PageRank~\cite{page1999pagerank} calculate node proximities by aggregating multi-hop transition probabilities using a descending weight sequence. Meanwhile, they set a teleport probability $\alpha$ returning to the initial state regularly, which can emphasize the importance of direct neighbors. Heat kernel PageRank is mainly used in local clustering, which equals to the terminating probability of a heat kernel random walk. The length of heat kernel random walk follows the poisson distribution with parameter $t$. Hence, the $t$-hop neighbors will be paid more attention instead of the direct neighbors compared with Personalized PageRank.
% %which limits the level of neighbors concerned during clustering tasks.  
% Besides, HITS measures the number of paths between two nodes, and view the nodes which can be reached fast and frequently as the high-proximity nodes. Elaborated illustrations of various graph analysis metrics can be found in Section~\ref{sec:intro}. 

% Graph learning tasks want to learn a good vector representation for each node. Using these learned vectors, they can further adopt machine learning methods to fix target problems end to end. In the learning process, these tasks need to find some good propagation techniques to transfer node features efficiently. Most of works adopt some graph analysis metric to do the propagation, because these metric aggregate the information of graph structures. Hence, choosing a high-quality graph analysis metric can heavily affect the representation result in learning tasks. 


%We present a new local push algorithm, which can decrease Backward Push operation's complexity to $\tilde{O}(\frac{1}{\delta})$ with $\tilde{O}(\delta \cdot \pi(s,t))$ estimation variance, where $\tilde{O}$ means $log$ factors omitted. Combined this new Backward Push with Monte-Carlo methods as FAST-PPR~\cite{Lofgren2014FASTPPR}  does, we can eventually accomplish a $\tilde{O}(\frac{1}{\sqrt{\delta}})$ time complexity approach. 
%This almost achieve the lower bound $O(\frac{1}{\sqrt{\delta}})$ of single-pair PPR calculation problem.




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
