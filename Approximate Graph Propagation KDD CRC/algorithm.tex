\vspace{-2mm}
\section{Basic Propagation}
\vspace{-1mm}
\label{sec:BPA}
In the next two sections, we present two algorithms to compute the graph propagation equation~\eqref{eqn:pi_gen} with the theoretical relative error guarantee in Definition~\ref{def:pro-relative}.   
%  Algorithm~\ref{alg:AGP-deter} computes $\vec{\hat{\pi}} = \sum_{i=0}^L w_i \cdot \left(\bm{D}^{-a}\cdot \bm{A} \cdot \bm{D}^{-b} \right)^i \cdot \vec{x}$, which is the propagation equation~\eqref{eqn:pi_gen} truncated at level $L$. The value of $L$ depends on the approximation quality. 
%Let $L_E = \sum_{i=0}^\infty w_i\cdot i = \sum_{i=0}^\infty Y_i$ be the average length of the propagation. 

\vspace{-0.5mm}
\header{\bf Assumption on graph signal $\vec{x}$.} 
% To simulate a column of the node feature matrix $\mathbf{X}$ in GNN,  the graph signal $\mathbf{x}$  may contain negative entries. However, 
For sake of simplicity, we assume the graph signal $\vec{x}$ is non-negative. We can deal with the negative entries in $\vec{x}$ by decomposing it into $\vec{x} \hspace{-0.5mm}=\hspace{-0.5mm} \vec{x}^{+} \hspace{-0.5mm}+\hspace{-0.5mm} \vec{x}^-$, where $\vec{x}^+$ only contains the non-negative entries of $\vec{x}$ and $\vec{x}^-$ only contains the negative entries of  $\vec{x}$. After we compute $\vec{\pi}^+\hspace{-0.5mm}=\hspace{-0.5mm}\sum_{i=0}^\infty w_i \cdot \left(\mathbf{D}^{-a}\mathbf{A} \mathbf{D}^{-b} \right)^i \hspace{-1mm} \cdot \hspace{-1mm}\vec{x}^+$ and $\vec{\pi}^{-}\hspace{-1mm}=\hspace{-1mm}\sum_{i=0}^\infty w_i \cdot \left(\mathbf{D}^{-a}\mathbf{A} \mathbf{D}^{-b} \right)^i\hspace{-0.5mm} \cdot\hspace{-0.5mm} \vec{x}^-$, we can combine  $\vec{\pi}^+$ and $\vec{\pi}^-$ to form $\vec{\pi}\hspace{-0.5mm} = \hspace{-0.5mm} \vec{\pi}^+ \hspace{-0.5mm}+\hspace{-0.5mm} \vec{\pi}^-$. We will also assume $\vec{x}$ is normalized, that is $\|\vec{x}\|_2 \hspace{-1mm}=\hspace{-0.5mm} 1$. 

\vspace{-0.5mm}
\header{\bf Assumptions on $w_i$.} To make the computation of Equation~\eqref{eqn:pi_gen} feasible, we first introduce several assumptions on the weight sequence $w_i$ for $i\in \{0,1,2, ...\}$. We assume $\sum_{i=0}^\infty w_i =1$. If not, we can perform propagation with $w_i' = w_i/\sum_{i=0}^\infty w_i$ and rescale the result by  $\sum_{i=0}^\infty w_i$. We also note that to ensure the convergence of Equation~\ref{eqn:pi_gen}, the weight sequence $w_i$ has to satisfy $\sum_{i=0}^\infty w_i \lambda_{max}^i < \infty$, where $\lambda_{max}$ is the maximum singular value of the propagation matrix $\mathbf{D}^{-a} \mathbf{A}\mathbf{D}^{-b}$. Therefore, we assume that for sufficiently large $i$, $w_i \lambda_{max}^i$ is upper bounded by a geometric distribution: 

\begin{assumption}\label{asm:L}
\vspace{-2mm}
There exists a constant $L_0$ and $\lambda < 1$, such that for any $i \ge L_0$, $w_i \cdot \lambda_{max}^i  \le \lambda^i$.
\end{assumption}
According to Assumption~\ref{asm:L}, to achieve the relative error in Definition~\ref{def:pro-relative}, we only need to compute the prefix sum $\vec{\pi}=\sum_{i=0}^L w_i \cdot \left(\mathbf{D}^{-a}\mathbf{A}\mathbf{D}^{-b} \right)^i \cdot \vec{x}$, where $L$ equals to $ \log_{\lambda} {\delta} = O\left( \log {1 \over \delta}\right)$.
This property is possessed by all proximity measures discussed in this paper. For example, PageRank and Personalized PageRank set $w_i=\alpha \left(1-\alpha\right)^i$, where $\alpha$ is a constant. Since the maximum eigenvalue of $ \mathbf{A}\mathbf{D}^{-1}$ is $1$, we have $\|\sum_{i=L+1}^\infty w_i\mathbf{A}\mathbf{D}^{-1} \cdot \vec{x}\|_2 \le \|\sum_{i=L+1}^\infty w_i\cdot \vec{x}\|_2 \le \sum_{i=L+1}^\infty \alpha \cdot \left(1-\alpha\right)^i =(1-\alpha)^{L+1}$. In the last inequality, we use the assumption on $\vec{x}$ that $\|\vec{x}\|_2=1$. If we set $L=\log_{1-\alpha} \delta=O\left(\log{\frac{1}{\delta}} \right)$, the remaining sum $\|\sum_{i=L+1}^\infty w_i\mathbf{A}\mathbf{D}^{-1} \cdot \vec{x}\|_2$ is bounded  by $\delta$.
We can prove similar bounds for HKPR, Katz, and transition probability as well.

% Thus, We can bound $\sum_{i=L}^\infty w_i=\sum_{i=L}^\infty \alpha \cdot \left(1-\alpha\right)^i =(1-\alpha)^L$ by $\delta$ when  we set $L=\log_{1-\alpha} \delta=O\left(\frac{1}{\alpha}\log{\frac{1}{\delta}} \right) = O\left(\log{\frac{1}{\delta}} \right)$. For heat kernel PageRank, $w_i=e^{-t} \cdot \frac{t^i}{i!}$ follows the Poisson distribution, which decays faster than the geometric distribution $w_i=\left(1-1/t\right)^i$~\cite{kloster2014heat} for $i > t$. Hence, $\sum_{i=L}^\infty e^{-t} \cdot \frac{t^i}{i!}\le \delta$ is guaranteed after $O\left(t\log{\frac{1}{\delta}} \right)$ iterations, where $t$ is a constant.  We can prove similar bounds for HKPR, Katz, and transition probability as well.

% \begin{assumption}\label{asm:L}
% Let $L$ denote the minimum number of levels such that $\sum_{i=L}^\infty w_i \le \delta$, then $L=O\left(\log{\frac{1}{\delta}} \right)$.
% \end{assumption}

% Assumption~\ref{asm:L} essentially states that we only need to exploit the first $L=O\left(\log{\frac{1}{\delta}} \right)$ levels to achieve a relative error of $\delta$. This assumption is well justified in various applications of the graph propagation. For example, PageRank and Personalized PageRank set $w_i=\alpha \cdot \left(1-\alpha\right)^i$, where $\alpha$ is a constant.  Thus, We can bound $\sum_{i=L}^\infty w_i=\sum_{i=L}^\infty \alpha \cdot \left(1-\alpha\right)^i =(1-\alpha)^L$ by $\delta$ when  we set $L=\log_{1-\alpha} \delta=O\left(\frac{1}{\alpha}\log{\frac{1}{\delta}} \right) = O\left(\log{\frac{1}{\delta}} \right)$. For heat kernel PageRank, $w_i=e^{-t} \cdot \frac{t^i}{i!}$ follows the Poisson distribution, which decays faster than the geometric distribution $w_i=\left(1-1/t\right)^i$~\cite{kloster2014heat} for $i > t$. Hence, $\sum_{i=L}^\infty e^{-t} \cdot \frac{t^i}{i!}\le \delta$ is guaranteed after $O\left(t\log{\frac{1}{\delta}} \right)$ iterations, where $t$ is a constant.  We can prove similar bounds for Katz and transition probability as well.


% e can always calculate the graph propagation with $\vec{\pi}=\|\vec{x}\|_1  \cdot \sum_{i=0}^\infty w_i \cdot \left(\mathbf{D}^{-a}\mathbf{A} \mathbf{D}^{-b} \right)^i \cdot \frac{\vec{x}}{\|\vec{x}\|_1 } $ to make this assumption valid. 



\header{\bf Basic Propagation.} 
As a baseline solution, we can compute the graph propagation equation~\eqref{eqn:pi_gen} by iteratively updating the propagation vector $\vec{\pi}$ via matrix-vector multiplications. Similar approaches have been used for computing PageRank, PPR, HKPR and Katz, under the names of Power Iteration or Power Method.  


In general, we employ matrix-vector multiplications to compute the summation of the first $L =O\left(\log {1\over \delta}\right)$ hops of equation~\eqref{eqn:pi_gen}: $\vec{\pi}=\sum_{i=0}^L w_i \cdot \left(\mathbf{D}^{-a}\mathbf{A}\mathbf{D}^{-b} \right)^i \cdot \vec{x}$.  To avoid the $O(nL)$ space of storing vectors $\left(\mathbf{D}^{-a}\mathbf{A} \mathbf{D}^{-b} \right)^i \cdot \vec{x}, i=0,\ldots, L$, we only use two vectors: the {\em residue and reserve vectors}, which are defined as follows. 


\begin{definition}\label{def:RQ-relation} [{\bf residue and reserve}]
Let $Y_i=\sum_{k=i}^\infty w_k, i=0,\ldots, \infty,$ denote the partial sum of the weight sequence. Recall that $Y_0 = \sum_{k=0}^\infty w_k = 1$.  At level $i$, the residue vector is defined as $\vec{r}^{(i)}=Y_i\cdot \left(\D^{-a}\A\D^{-b} \right)^i \cdot \vec{x}$;  The reserve vector is defined as $\vec{q}^{(i)}=\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}=w_i \cdot \left(\D^{-a}\A\D^{-b} \right)^i \cdot \vec{x}$. 
\end{definition}

% \begin{definition}\label{def:RQ-relation} [{\bf residue and reserve vectors}]
% At level $i$, the residue vector $\vec{r}^{(i)}$ and reserve vector $\vec{q}^{(i)}$ are defined as follows:
% \begin{itemize}
%   \item $\vec{r}^{(i)}=Y_i\cdot \left(\bm{D}^{-a}\cdot \bm{A} \cdot \bm{D}^{-b} \right)^i \cdot \vec{x};$
%   \item $\vec{q}^{(i)}=\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}=w_i \cdot \left(\bm{D}^{-a}\cdot \bm{A} \cdot \bm{D}^{-b} \right)^i \cdot \vec{x}, $
% \end{itemize}
% \end{definition}

%Intuitively, for each node $u\in V$ and level $i \ge 0$, the residue $\vec{r}^{(i)}(u)$ denotes the energy to be propagated to $u$'s neighbors in the next level, and the reserve $\vec{q}^{(i)}(u)$ denotes the energy that stays at node $u$ in level $i$. 
Intuitively, for each node $u\in V$ and level $i \ge 0$, the residue $\vec{r}^{(i)}(u)$ denotes the probability mass to be distributed to node $u$ at level $i$, and the reserve $\vec{q}^{(i)}(u)$ denotes the probability mass that will stay at node $u$ in level $i$ permanently. 
By Definition~\ref{def:RQ-relation}, the graph propagation equation~\eqref{eqn:pi_gen} can be expressed as $\vec{\pi}=\sum_{i=0}^\infty \vec{q}^{(i)}.$
Furthermore, the residue vector $\vec{r}^{(i)}$ satisfies the following recursive formula:
\begin{align}\label{eqn:iteration}
	\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\D^{-a}\A \D^{-b} \right)\cdot \vec{r}^{(i)}. 
\end{align} 
%$\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\bm{D}^{-a}\cdot \bm{A} \cdot \bm{D}^{-b} \right)\cdot \vec{r}^{(i)}$. 
We also observe that the reserve vector  $\vec{q}^{(i)}$ can be derived from the residue vector $\vec{r}^{(i)}$ by $\vec{q}^{(i)}=\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}$.  Consequently, given a predetermined level number $L$, we can compute the graph propagation equation~\eqref{eqn:pi_gen} by iteratively computing the residue vector $\vec{r}^{(i)}$ and reserve vector $\vec{q}^{(i)}$ for $i=0,1,...,L$.

\begin{algorithm}[t]%[ht]
\begin{small}
    \caption{Basic Propagation Algorithm\label{alg:AGP-deter}}
	\KwIn{Undirected graph $G=(V,E)$, graph signal vector $\vec{x}$, weights $w_i$,  number of levels $L$\\}
	\KwOut{the estimated propagation vector $\hat{\vec{\pi}}$\\}
	$\vec{r}^{(0)} \gets \vec{x}$\;
	%$\hat{\pi} \gets w_0\cdot \hat{P}^{(0)}$\;
	\For{$i=0$ to $L-1$}{

		\For{each $u \in V$ with nonzero $\vec{r}^{(i)}(u)$}{
			\For{each $v\in N_u$ }{
				$\vec{r}^{(i+1)}(v) \gets \vec{r}^{(i+1)}(v) + \left(\frac{Y_{i+1}}{Y_i} \right) \cdot \frac{\vec{r}^{(i)}(u)}{d_v^{a} \cdot d_u^b}$;
			}
			$\vec{q}^{(i)}(u) \gets \vec{q}^{(i)}(u)+\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}(u)$\;
		}
		$\hat{\vec{\pi}} \gets \hat{\vec{\pi}} +\vec{q}^{(i)}$ and empty $\vec{r}^{(i)},\vec{q}^{(i)}$\;
	}	
	 $\vec{q}^{(L)}=\frac{w_L}{Y_L}\cdot \vec{r}^{(L)}$ and $\hat{\vec{\pi}} \gets \hat{\vec{\pi}} +\vec{q}^{(L)}$\;
	\Return $\hat{\vec{\pi}}$\;
	%\vspace{-3mm}
\end{small}
\end{algorithm}

Algorithm~\ref{alg:AGP-deter} illustrates the pseudo-code of the basic iterative propagation algorithm.  We first set $\vec{r}^{(0)}=\vec{x}$ (line 1). 
%For $i$ from $0$ to $L-1$, we compute $\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\D^{-a}\A\D^{-b} \right)\cdot \vec{r}^{(i)}$ by pushing an energy of $\left(\frac{Y_{i+1}}{Y_i} \right) \cdot \frac{\vec{r}^{(i)}(u)}{d_v^{a} \cdot d_u^b}$ to each neighbor $v$ of each node $u$ (lines 2-5). 
For $i$ from $0$ to $L-1$, we compute $\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\D^{-a}\A\D^{-b} \right)\cdot \vec{r}^{(i)}$ by pushing the probability mass  $\left(\frac{Y_{i+1}}{Y_i} \right) \cdot \frac{\vec{r}^{(i)}(u)}{d_v^{a} \cdot d_u^b}$ to each neighbor $v$ of each node $u$ (lines 2-5). 
Then, we set $\vec{q}^{(i)}=\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}$ (line 6), and aggregate $\vec{q}^{(i)}$ to $\hat{\vec{\pi}}$ (line 7). We also empty $\vec{r}^{(i)},\vec{q}^{(i)}$ to save memory. After all $L$ levels are processed, we transform the residue of level $L$ to the reserve vector by updating $\hat{\vec{\pi}}$ accordingly (line 8). We return $\hat{\vec{\pi}}$  as an estimator for the graph propagation vector $\vec{\pi}$ (line 9). 

Intuitively, each iteration of Algorithm~\ref{alg:AGP-deter}  computes the matrix-vector multiplication $\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\D^{-a}\A\D^{-b}\right)\cdot \vec{r}^{(i)}$, where $\D^{-a}\A\D^{-b}$ is an  $n\times n$ sparse matrix with $m$ non-zero entries. Therefore, the cost of each iteration of Algorithm~\ref{alg:AGP-deter}  is $O(m)$. To achieve the relative error guarantee in Definition~\ref{def:pro-relative}, we need to set $L=O\left(\log{\frac{1}{\delta}} \right)$, and thus the total cost becomes $O\left(m\cdot \log{\frac{1}{ \delta}}\right)$.  Due to the logarithmic dependence on $\delta$, we use Algorithm~\ref{alg:AGP-deter} to compute high-precision proximity vectors as the ground truths in our experiments. 
%Finally, in the setting of Graph Neural Network, there are $d$ different graph signals $\vec{x}$ to be propagated, where $d$ is the dimension of the node feature vector. 
However, the linear dependence on the number of edges $m$ limits the scalability of Algorithm~\ref{alg:AGP-deter} on large graphs. In particular, in the setting of Graph Neural Network, we treat each column of the feature matrix $\X \in \mathcal{R}^{n \times d}$ as the graph signal $\bm{x}$ to do the propagation. 
%However, in the setting of Graph Neural Network, we treat each column of the feature matrix $\X \in \mathcal{R}^{n \times d}$ as the graph signal $\bm{x}$ to do the propagation. 
Therefore, Algorithm~\ref{alg:AGP-deter} costs $O\left(md \log  \frac{1}{\delta}\right)$ to compute the representation matrix $\mathbf{Z}$. Such high complexity limits the scalability of the existing GNN models.

% According to Assumption~\ref{asm:L}, the Basic Propagation Algorithm can achieve high-precision results with a logarithmic number of iterations.  
% However, the scalability of Algorithm~\ref{alg:AGP-deter} is unsatisfactory. Recall in each iteration, Algorithm~\ref{alg:AGP-deter} conducts the propagation from every node with nonzero residue, no matter how small the residue is. According to the small world theory~\cite{milgram1967small}, the propagation, even from only one node, can quickly reach almost every nodes on the graph in only a few iterations. Therefore,  Algorithm~\ref{alg:AGP-deter} will perform a global propagation with an $O(m)$ cost for every iteration, leading to an $O\left(m \log  \frac{1}{\delta}\right)$ total cost.



% $w_i=\beta^i$ and $\beta$ is smaller than the maximum eigenvalue of the traverse adjacent matrix $\bm{A}^\top$. After $L$ iterations with $L\ge \log_{\beta}\left((1-\beta)\cdot \delta\right)=O\left(\log{\frac{1}{\delta}} \right)$, the Katz index achieves convergence.  Finally, it is proved in \cite{peserico2009score} that HITS can becomes converged within $O\left(\log{\frac{1}{\delta}} \right)$ iterations. 




% \header{\bf Analysis.} The following Theorem provides a time complexity for Algorithm~\ref{alg:AGP-deter} under  Assumption~\ref{asm:L}. 

% \begin{theorem}
% \label{thm:basic}	
% 	By setting $L = c\log{\frac{1}{\delta}}$ for some sufficiently large constant $c$, Algorithm~\ref{alg:AGP-deter} returns an estimator $\hat{\vec{\pi}}$ of the true propagation vector $\vec{\pi}$, such that for any node $v$ with $\vec{\pi}(v)>\delta$, $\left|\vec{\pi}(v)-\hat{\vec{\pi}}(v)\right| \leq \frac{1}{10} \cdot \vec{\pi}(v)$. 
% 	The time cost of Algorithm~\ref{alg:AGP-deter} is $O\left(m\cdot \log{\frac{1}{\delta}}\right)$, where $m$ denotes the number of edges on the graph. 
% \end{theorem}

% For ease of presentation, we defer all proofs to the technical report~\cite{TechnicalReport}. 






% \noindent{\bf Remarks.} The Basic Propagation Algorithm assembles different propagation structures into a generalized version. By manipulating the parameters $a$, $b$, $w_i$ ($i\ge 0$), and $\vec{x}$, we can easily transform Algorithm~\ref{alg:AGP-deter} to a specific power method of any propagation method, such as PageRank, PPR, HKPR, Katz, HITS as mentioned in Section~\ref{sec:pre}. Despite the Basic Propagation Algorithm doesn't propose a totally new propagation way, it presents a general view to summarize various graph propagation approaches. By Algorithm~\ref{alg:AGP-deter}, we can make an overall discussions towards these methods.  


% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\sum_{i=L}^\infty w_i \le \delta. 
% 	\end{aligned}
% \end{equation}
%For each node $t\in V$ with $\pi(t)\ge \delta d_{t} \cdot \sum_i^{\infty} Y_i$, Algorithm~\ref{alg:AGP-deter} returns the estimated propagation value $\hat{\pi}(t)$ satisfies that
	%$$\left|\pi(t)-\epi(t)\right| \leq \frac{1}{10} \cdot \pi(t).$$ 
	
% %Note that equation~\eqref{eqn:pi_gen} can be viewed as the sum of the $\ell$-hop propagation results from level $0$ to infinity. Thus, we can basically use power method to iteratively update vector $\pi$ level by level. 
% Note that the sum of all weighted parameters $Y_0=\sum_{i=0}^\infty w_i$ may not equal $1$ in the generalized propagation structure. So we divide $Y_0$ in the definition equation of $\vec{r}^{(i)}$ and $\vec{q}^{(i)}$ and call them normalized residue and reserve vectors. 

%Recall that in Algorithm~\ref{alg:AGP-deter}, after each propagation from node $u$ at level $\ell$, the estimated residue $\hat{R}^{(\ell)}(u)$ will be set as $0$ after the propagation. Here let $\vec{r}^{(\ell)}_m(u)$ denote the maximum value of $\vec{r}^{(\ell)}(u)$ before it is set as $0$. 
%For each $i\ge 0$, the relations among the vectors $\vec{r}^{(i)}$, $\vec{q}^{(i)}$, and the propagation vector $\pi$ can be summarized as below. 

% \begin{algorithm}[h]
% 	\caption{Basic Propagation Algorithm (vector form)\label{alg:AGP-deter-M}}
% 	\KwIn{Undirected graph $G=(V,E)$, graph signal vector $\vec{x}$, weights $w_i$, relative error threshold $\delta$\\}
% 	\KwOut{the estimated propagation vector $\hat{\vec{\pi}}$, number of levels $L$\\}
% 	$\vec{r}^{(i)} \gets \vec{0}$, $\vec{q}^{(i)} \gets \vec{0}$, for $i=0,1,...,L$; $\vec{r}^{(0)} \gets \vec{x}$\;
% 	%$\hat{\pi} \gets w_0\cdot \hat{P}^{(0)}$\;
% 	\For{$i=0$ to $L-1$}{
% 		%\For{each $u \in V$ with $\hat{R}^{(i)}(u)\ge Y_i\cdot \delta \cdot d_u^r$}{
% 			$\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\bm{D}^{-a}\cdot \bm{A} \cdot \bm{D}^{-b} \right)\cdot \vec{r}^{(i+1)}$\;
% 			$\vec{q}^{(i)}=\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}$ and $\hat{\vec{\pi}} += \vec{q}^{(i)}$\;
			
% 	}	
% 	\Return $\hat{\vec{\pi}} $ \;
% \end{algorithm}



% we can rewrite the graph propagation equation~\eqref{eqn:pi_gen} as $\vec{\pi}=\sum_{i=0}^\infty \vec{q}^{(i)}$. 


% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\vec{\pi}=\sum_{i=0}^\infty \vec{q}^{(i)}.
% 	\end{aligned}
% \end{equation}
% Because $\vec{r}^{(i+1)}=\frac{Y_{i+1}}{Y_i}\cdot\left(\bm{D}^{-a}\cdot \bm{A} \cdot \bm{D}^{-b} \right)\cdot \vec{r}^{(i+1)}$, for each level $i\ge 0$, we don't have to repeat the multiplication $i+1$ times to compute $\vec{r}^{(i+1)}$. 

% Furthermore, the reserve vector $\vec{q}^{(i)}$ can be computed based on the relationships between $\vec{r}^{(i)}$ and $\vec{q}^{(i)}$ that 
% \begin{align}\label{eqn:iteration-q}
% \vec{q}^{(i)}=\frac{w_i}{Y_i}\cdot \vec{r}^{(i)}. 	
% \end{align}
% By equation~\eqref{eqn:iteration} and ~\eqref{eqn:iteration-q}, we can present the Basic Propagation Algorithm based on the following assumption.  



% Consequently, we can compute the the graph propagation equation~\eqref{eqn:pi_gen} with the following proceedure:


% %where $\tilde{N}(v)=N_{in}(v)$ if $\bm{\tilde{A}}=\bm{A}^\top$ in the generalized equation~\eqref{eqn:pi_gen}, or $\tilde{N}(v)=N_{out}(v)$ if $\bm{\tilde{A}}=\bm{A}$. 





% Instead, the $(i+1)$-hop residue $\vec{r}^{(i+1)}(v)$ can be derived iteratively that
% \begin{equation}\label{eqn:iteration}
% 	\begin{aligned}
% 		\vec{r}^{(i+1)}(v)=\frac{Y_{i+1}}{Y_i}\cdot \hspace{-1mm}\sum_{u \in N(v)}\frac{\vec{r}^{(i)}(u)}{d_v^a\cdot d_u^b}. 
% 	\end{aligned}
% \end{equation}




%Consequently, the error bringing from the truncation at level $L=\log{1 \over \delta}$ can be bounded by $\delta$, following the approximation propagation with the relative error threshold $\delta$ of Algorithm~\ref{alg:AGP-deter}. 





% Algorithm~\ref{alg:AGP-deter} illustrates the pseudo-code of the Basic Propagation Algorithm. 
% %$\vec{r}^{(\ell)}$ and $\vec{q}^{(\ell)}$ denote the vector of each node's $\ell$-hop residue and reserve. In detail, the reserve $\vec{q}^{(\ell)}(t)$ denotes the probability mass that the propagation starting from the graph signal vector $\vec{x}$, terminates at node $t$ at exactly $\ell$ steps. And the residue $\vec{r}^{(\ell)}(t)$ denotes the probability mass to be distributed at node $t$ at exactly $\ell$ steps. 
% Considering an undirected graph $G=(V,E)$, a graph signal vector $\vec{x}$, a weighted sequence $w_i$ for $i\ge 0$, and a relative error threshold $\delta$, %and a maximum number of levels $L$, 
% we first set the maximum level to do iteration as $L=\log{1 \over \delta}$ (Line 1). Initialize the estimated residue and reserve vectors $\hat{R}^{(i)}$ and $\hat{Q}^{(i)}$ as zero vectors for $i=0,1,...,L$, except $\hat{R}^{(0)}=\vec{x}$ (Line 2-3). Then we iteratively conduct the propagation from level $0$ to level $L-1$. At level $i$, we pick those nodes $u$ whose estimated residue $\hat{R}^{(i)}(u)>0$. % satisfies that $\hat{R}^{(i)}(u)\ge \frac{Y_i}{Y_{i+1}}\cdot \delta \cdot d_u^r$ (Line 4), where $Y_i$ is defined as: 
% %\begin{align}\label{eqn:Y-def}
% %	Y_i=\sum_{k=i}^\infty w_k. 
% %\end{align}
% In each propagation operation, node $u$ propagates $\left(1-\frac{w_i}{Y_i} \right)$ fraction of its residue $\hat{R}^{(i)}(u)$ to its neighbors. %the residue of its neighbors.
% %If the $\tilde{A}=A^\top$ in the propagation structure, such as the single-source PPR query, HITS, Katz, and HKPR, 
% Specifically, for each neighbor $v\in N(u)$, we add $\left(1-\frac{w_i}{Y_i} \right) \cdot \frac{\hat{R}^{(i)}(u)}{d^a_v \cdot d^b_u}$ to the residue $\hat{R}^{(i+1)}(v)$ (Line 6-7). %While if $\tilde{A}=A$ such as the single-target PPR query, we update the residue for every in-neighbor of node $u$. 
% %The remained $\frac{w_i}{Y_i}$ fraction is added to its reserve $\hat{Q}^{(i)}(u)$. 
% After every neighbor $v$ are processed, update the reserve $\hat{Q}^{(i)}(u)$ by the remained $\frac{w_i}{Y_i}$ fraction of $\hat{R}^{(i)}(u)$, and then set $\hat{R}^{(i)}(u)$ as $0$ (Line 8-9). When the first $L$ levels have been propagated, return the sum of $\hat{Q}^{(i)}$ ($i\in [0,L]$) as the estimator of propagation result $\pi$. 




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:
