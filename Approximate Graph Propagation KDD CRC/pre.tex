\section{Preliminary and Related Work} \label{sec:pre}




\begin{table} [t]
	\centering
	\renewcommand{\arraystretch}{1.3}
	\begin{small}
		\tblcapup
		\caption{Table of notations.}\label{tbl:def-notation}
		\vspace{-4mm}
		%\tblcapdown
		%p{2.3in}
		\begin{tabular} {|l|p{2.3in}|} \hline
			{\bf Notation} &  {\bf Description}  \\ \hline
			$G=(V,E)$ & undirected graph with vertex and   edge sets $V$ and $E$ \\ \hline
			$n, m$      &   the numbers of nodes and edges in $G$                            \\ \hline
			%$N_{in}(u), N_{out}(u)$	& 	the in/out neightbor set of node $u$	\\ \hline
			%$d_{in}(u), d_{out}(u)$ & the in/out degree of node $u$ \\ \hline
			$\mathbf{A}$, $\mathbf{D}$  & the adjacency matrix and degree matrix of $G$\\ \hline
			%$\mathbf{D}$ & the diagonal degree matrix of $G$\\ \hline
			$N_u$, $d_u$	& 	the neighbor set and the degree of node $u$\\ \hline
			%$N_{i}(u), N_{o}(u)$	& 	the in-/out-neighbor set of node $u$	\\ \hline
			%$d_u$ & the degree of node $u$ on undirected graph\\ \hline
			%$d_{i}(u), d_{o}(u)$ & the in-/out-degree of node $u$ \\ \hline
			$a,b$ & the Laplacian parameters\\ \hline
			$\vec{x}$ & the graph signal vector in $\mathcal{R}^n$, $\left\| \vec{x} \right\|_{2}=1$ \\ \hline
			$w_i, Y_i$ & the $i$-th weight  and partial sum $Y_i=\sum_{k=i}^\infty w_k$\\ \hline
			%	$L_E = \sum_{i=0}^\infty i w_i $  & the average propagation length\\ \hline
			%$\vec{w}$ & the weighted vector with $w_i$ as the $i_{th}$ entry\\ \hline
			$\vec{\pi},\hat{\vec{\pi}}$ & the true and estimated propagation vectors in $\mathcal{R}^n$\\ \hline
			$\vec{r}^{(i)},\hat{\vec{r}}^{(i)}$ & the true and estimated $i$-hop residue vectors in $\mathcal{R}^n$\\ \hline
			$\vec{q}^{(i)},\hat{\vec{q}}^{(i)}$ & the true and estimated $i$-hop reserve vectors in $\mathcal{R}^n$\\ \hline
			%$\e_r,\delta$ & the relative error and threshold \\ \hline
			$\delta$ & the relative error threshold \\ \hline
			$\tilde{O}$ & the Big-Oh natation ignoring the log factors \\ \hline
			%$\pi_{\ell}(s,t), \epi_{\ell}(s,t)$	& 	the exact and estimated $\ell$-hop PPR value of node $t$ with respect to $s$. \\ \hline
			%$\pi_{\ell}(s,t), r_{\ell}(s,t)$	& 	the reserve and residue of $t$ during $\ell$-hop PPR push from $s$.\\ \hline
			%$X_{\ell}(u,v)$	&	the backward push's increments from node $u$ (in level $\ell$) to node v (in level $\ell+1$)\\ \hline
			%$C_{\ell}(u,v)$	&	cost in the backward push from node $u$ (in level $\ell$) to node $v$	\\ \hline
			%$r_s^f(u), r_t^b(u)$	&	the Node Income	of $u$ in forward / backward Push start from node $s / t$\\ \hline
			%$r_s^f(u,v), r_t^b(u,v)$	&	the Edge Saving	of edge ($u,v$) in forward / backward Push start from node $s / t$\\ \hline
			%$p_s^f(u,v), p_t^b(u,v)$	&	the Edge Expense of edge ($u,v$) in forward / backward Push start from node $s / t$\\ \hline			
			%$\alpha$	& the probability that a random walk terminates at a step \\ \hline
			%$c$          &   the decay factor in the definition of SimRank                   \\ \hline
			%$\e, \e_{min}$         &   additive error parameter and error required for exactness ($\e_{min} = 10^{-7}$)          \\ \hline
			% $\e_r$         &   the maximum relative error allowed in top-$k$ SimRank queries
			%$P$, $D$   & the transition matrix and the diagonal correction matrix\\ \hline
			%$\vec{\pi}_i, \vec{\pi}_i^\ell,$   & the Personalized PageRank and $\ell$-hop Personalized PageRank vectors of node $v_i$\\ \hline
			%$ \vec{h}_i^\ell$   &  the $\ell$-hop Hitting Probability vector of $v_i$\\	\hline
			
			%     $\rf(s,t)$, $\pif(s,t)$ & The reserve and residue of $t$ from $s$ in the forward search \\
			%     \hline
			%     $\frsum$ & The sum of all nodes' residues during in the forward
			%                search from $s$\\
			% \hline
			%   $h^{\l}(v_i, v_j)$ & the hitting probability (HP) from node $v_i$ to node $v_j$ at step $\l$ (see Section~\ref{sec:our-overview}) \\ \hline
		\end{tabular}
		\vspace{-3mm}
	\end{small}
\end{table}


In this section, we provide a detailed discussion on how the graph propagation equation~\eqref{eqn:pi_gen} models various node proximity measures. %We will also discuss how approximate graph propagation improves the scalability of  various applications, including node proximity computation, local clustering, and graph neural networks. 
Table~\ref{tbl:def-notation} summarizes the notations used in this paper. 


\header{\bf  Personalized PageRank (PPR) }~\cite{page1999pagerank} is  developed by Google to rank web pages on the world wide web, with the intuition that "a page is important if it is referenced by many pages, or important pages". Given an undirected graph $G=(V,E)$ with $n$ nodes and $m$ edges and a {\em teleporting probability distribution} $\vec{x}$ over the $n$ nodes, the PPR  vector $\vec{\pi}$ is the solution to the following equation:
\vspace{-1mm}
\begin{equation}\label{eqn:PageRank_definition}
	\begin{aligned}
	\vspace{-2mm}
		\vec{\pi}=\left( 1-\alpha \right) \cdot \mathbf{A}\mathbf{D}^{-1} \cdot \vec{\pi}+ \alpha\vec{x}.
	\end{aligned}
	\hspace{-2mm}
\end{equation}
The unique solution to Equation~\eqref{eqn:PageRank_definition} is given by $\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A} \mathbf{D}^{-1} \right)^i \cdot \vec{x}.$
% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A} \cdot\mathbf{D}^{-1} \right)^i \cdot \vec{x},
% 	\end{aligned}
% \end{equation}
%  which can be expressed by the graph propagation if we set  $a=0, b=1, w_i=\alpha \cdot \left(1-\alpha\right)^i$, and $\vec{x}=\vec{e}_s $ (or $\frac{1}{n}\cdot \vec{1}$ for PageRank) in equation~\eqref{eqn:pi_gen}.


%  PageRank and Personalized PageRank reflects the structural importance of each node, and  is widely used in graph mining tasks~\cite{Liben-NowellK03,gupta2013wtf} and graph representation learning tasks~\cite{ou2016asymmetric,tsitsulin2018verse,zhou2017scalable,Klicpera2018APPNP}. Consequently, the efficient computation of the PPR vector has been extensively studied for the past decades. Due to space limits, we only discuss a few works that are closely related to our AGP framework. 
 
The efficient computation of the PPR vector has been extensively studied for the past decades. A simple algorithm to estimate PPR is the Monte-Carlo sampling~\cite{Fogaras2005MC}, which stimulates adequate random walks from the source node $s$ generated following $\bm{x}$ and uses the percentage of the walks that terminate at node $v$ as the estimation of $\vec{\pi}(v)$.  Forward Search~\cite{FOCS06_FS} conducts deterministic local pushes from the source node $s$ to find nodes with large PPR scores. FORA~\cite{Wang2017FORA} combines Forward Search with the Monte Carlo method to improve the computation efficiency. TopPPR~\cite{wei2018topppr} combines Forward Search, Monte Carlo, and Backward Search~\cite{lofgren2013personalized} to obtain a better error guarantee for  top-$k$ PPR estimation. ResAcc~\cite{lin2020index} refines FORA by accumulating probability masses before each push.  However, these methods only work for graph propagation with transition matrix $\mathbf{A} \mathbf{D}^{-1}$. For example, the Monte Carlo method simulates random walks to obtain the estimation, which is not possible with the general propagation matrix $\mathbf{D}^{-a}\mathbf{A}\mathbf{D}^{-b}$. 
 
Another line of research~\cite{lofgren2013personalized,wang2020RBS} studies the {\em single-target PPR}, which asks for the PPR value of every node to a given target node $v$ on the graph.  The single-target PPR vector for a given node $v$ is defined by the slightly different formula: %$\vec{\pi} =\left( 1-\alpha \right) \cdot \left(\mathbf{D}^{-1}\mathbf{A} \right)\cdot \pi_v+\alpha \vec{e}_v.$
\vspace{-2mm}
\begin{equation}\label{eqn:st_PPR_definition}
\vspace{-2mm}
 	\begin{aligned}
 		\vec{\pi} =\left( 1-\alpha \right) \cdot \left(\mathbf{D}^{-1} \mathbf{A} \right)\cdot \vec{\pi}+\alpha \vec{e}_v.  
	\end{aligned}
 \end{equation}
Unlike the single-source PPR vector, the single-target PPR vector $\vec{\pi}$ is not a probability distribution, which means $\sum_{s\in V}\vec{\pi}(s)$ may not equal to $1$. 
We can also derive the unique solution to Equation~\eqref{eqn:st_PPR_definition} by $\vec{\pi} =\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-1}\mathbf{A} \right)^i \cdot \vec{e}_v$.
% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\vec{\pi} =\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-1} \cdot \mathbf{A} \right)^i \cdot \vec{e}_v, 
% 	\end{aligned}
% \end{equation}
%  which can be formulated as the graph propagation~\eqref{eqn:pi_gen} by setting $a=1,b=0, w_i=\alpha \left( 1-\alpha \right)^i, \vec{x}=\vec{e}_v$. 

\header{\bf Heat kernal PageRank} is proposed by~\cite{chung2007HKPR} for high quality community detection. For each node $v\in V$ and the seed node $s$, the heat kernel PageRank (HKPR) $\vpi(v)$ equals to the probability that a heat kernal random walk starting from node $s$ ends at node $v$. The length  $L$ of the random walks follows the Poisson distribution with parameter $t$, i.e. $\Pr[L = i] = \frac{e^{-t}t^i}{i!}, i=0,\ldots, \infty$. Consequently, the HKPR vector of a given node $s$ is defined as $\vec{\pi}=\sum_{i=0}^\infty \frac{e^{-t}t^i}{i!}\cdot \left(\mathbf{A} \mathbf{D}^{-1}\right)^i\cdot \vec{e}_s$, 
%\vspace{-1mm}
%\begin{equation}
%\begin{aligned}\label{eqn:definition_HKPR}
%\vspace{-1mm}
%	\vec{\pi}=\sum_{i=0}^\infty \frac{e^{-t}t^i}{i!}\cdot \left(\mathbf{A} \mathbf{D}^{-1}\right)^i\cdot \vec{e}_s,
%\end{aligned}	
%\end{equation}
where $\vec{e}_s$ is the one-hot vector with $\vec{e}_s(s)=1$.  
%Compared with Personalized PageRank, the length of random heat kernel PageRank
%Equation~\eqref{eqn:definition_HKPR} 
This equation fits in the framework of our generalized propagation equation~\eqref{eqn:pi_gen} if we set $a=0,b=1, w_i=\frac{e^{-t}t^i}{i!}$, and $\vec{x}=\vec{e}_s$. Similar to PPR, HKPR can be estimated by the Monte Carlo method~\cite{chung2018computing,yang2019TEA} that simulates random walks of Possion distributed length. HK-Relax~\cite{kloster2014heat} utilizes Forward Search to approximate the HKPR vector. TEA~\cite{yang2019TEA} combines Forward Search with  Monte Carlo for a more accurate estimator.

\header{\bf Katz index}~\cite{katz1953Katz} is another popular proximity measurement to evaluate relative importance of nodes on the graph. Given two node $s$ and $v$, the Katz score between $s$ and $v$ is characterized by the number of reachable paths from $s$ to $v$. Thus, the Katz vector for a given source node $s$ can be expressed as $\vec{\pi}=\sum_{i=0}^\infty \mathbf{A}^{i} \cdot \vec{e}_s,$ where $\mathbf{A} $ is the adjacency matrix and $\vec{e}_s$ is the one-hot vector with $\vec{e}_s(s)\hspace{-1mm} =\hspace{-1mm} 1$. However, this summation may not converge due to the large spectral span of $\mathbf{A}$. A commonly used fix up is to apply a penalty of $\beta$ to each step of the path, leading to the following definition: $\vec{\pi}=\sum_{i=0}^\infty \beta^i \cdot \mathbf{A}^{i} \cdot \vec{e}_s$. 
%\vspace{-1mm}
%\begin{align}~\label{eqn:Katz_defintion}
%\vspace{-1mm}
%	\vec{\pi}=\sum_{i=0}^\infty \beta^i \cdot \mathbf{A}^{i} \cdot \vec{e}_s. 
%\end{align}
To guarantee convergence, $\beta$ is a constant that set to be smaller than $\frac{1}{\lambda_1}$, where $\lambda_1$ is  the largest eigenvalue of the adjacency matrix $\mathbf{A}$. 
% Let $a=b=0, w_i=\beta^i, \vec{x}=\vec{e}_s$, our generalized propagation equation~\eqref{eqn:pi_gen} can also be used to compute the Katz index.
Similar to PPR, the Katz vector can be computed by iterative multiplying $\vec{e}_s$ with $\mathbf{A}$, which runs in $\Tilde{O}(m+n)$ time~\cite{foster2001faster}.  Katz has been widely used in graph analytic and learning tasks such as link prediction~\cite{Liben2003link} and graph embedding~\cite{ou2016asymmetric}. However, the $\Tilde{O}(m+n)$ computation time limits its scalability on large graphs. 


%\subsection{Concrete Applications} \label{sec:con-applications}

% \subsection{Node Importance and Proximity}
% The problem of efficiently computing various node importance and proximity measures has been extensively studied in both database and data mining communities for the past ten years. Among them, transition probabilities, PageRank, and Personalized PageRank~\cite{page1999pagerank}, heat kernel PageRank, and Katz~\cite{katz1953Katz} are the most widely used proximity measures. 
% In the following, we review each of these proximity measures and show how they fit in the AGP framework. 

% \header{\bf Transition probability} is a natural proximity measure on graphs. Given a source node $s$ and a level number $L$, the $L$-th transition probability of a node $v$  is the probability that a random walk from the source node $s$ visits $v$ at the $L$-th step. The vector form of the $L$-th transition probability is given by Equation~\eqref{eqn:transition_definition}.
% \begin{equation}\label{eqn:transition_definition}
% 	\begin{aligned}
% 		\vec{\pi}= \left(\mathbf{A} \mathbf{D}^{-1} \right)^L \cdot \left(\vec{e}_s \right),
% 	\end{aligned}
% \end{equation}
% where $\mathbf{A} \mathbf{D}^{-1}$ is the transition probability matrix and $\vec{e}_s$ is the one-hot vector with $\vec{e}_s(s)=1$ and $\vec{e}_s(x)=0, x\neq s$. 
% The transition probability vector 
% is a special case of the graph propagation equation~\eqref{eqn:pi_gen} if we set  $a=0, b=1, w_L=1, w_i=0,i=0,\ldots, L-1$, and $\vec{x}=\vec{e}_s$. 

% The transition probability vector can be computed exactly by iteratively multiplying the transition matrix $\mathbf{A} \mathbf{D}^{-1}$ to the one-hot vector $\vec{e}_s$, This algorithm takes $O(mL)$ time and is not scalable on large graphs. Spielman and Teng~\cite{Teng2004Nibble} proposes to truncate the node with small probabilities in each layer to speedup the computation. However, as we shall see in Section~\ref{sec:dp}, such truncation may lead to unbounded error in terms of approximation quality.

% \header{\bf PageRank and Personalized PageRank (PPR) }~\cite{page1999pagerank} are developed by Google to rank web pages on the world wide web, with the intuition that "a page is important if they are referenced by many pages, or important pages". For simplicity, we will formulate PageRank as a special case of PPR. 
% More specifically, given an undirected graph $G=(V,E)$ with $n$ nodes and $m$ edges and a {\em teleporting probability distribution} $\vec{x}$ over the $n$ nodes, the PPR  vector $\vec{\pi}$ is the solution to the following equation:
% \begin{equation}\label{eqn:PageRank_definition}
% 	\begin{aligned}
% 		\vec{\pi}=\left( 1-\alpha \right) \cdot \mathbf{A}\cdot\mathbf{D}^{-1} \cdot \vec{\pi}+ \alpha\cdot \vec{x}.
% 	\end{aligned}
% \end{equation}
% Intuitively, the PPR vector $\vec{\pi}$ is the stationary probability distribution of the {\bf random walk with teleport}. The random walk starts with the distribution $\vec{x}$ and, at each step, moves to a random neighbor of the current node with probability $1- \alpha$, or teleports to a random node according to distribution $\vec{x}$ with probability $\alpha$. For PageRank, the vector $\vec{x}$ is set to be $\frac{1}{n}\cdot \vec{1} $, which means the random walk will teleport to a uniformly selected node with probability $\alpha$ at each step. For Personalized PageRank of a given node $s$, the distribution is set to be the one-hot vector $\vec{e}_s$, which means the random walk will teleport back to $s$ with probability $\alpha$ at each step. 




% The unique solution to equation~\eqref{eqn:PageRank_definition} is given by the following formula:
% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A} \cdot\mathbf{D}^{-1} \right)^i \cdot \vec{x},
% 	\end{aligned}
% \end{equation}
%  which can be expressed by the graph propagation if we set  $a=0, b=1, w_i=\alpha \cdot \left(1-\alpha\right)^i$, and $\vec{x}=\vec{e}_s $ (or $\frac{1}{n}\cdot \vec{1}$ for PageRank) in equation~\eqref{eqn:pi_gen}.    
 


 














 
% \subsection{Local Clustering with proximity vectors} 
% \label{subsec:cluserting-pre}
% \header{\bf Local Clustering with proximity vectors.}
% Given an undirected graph $G=(V,E)$ and a seed node $s$, graph clustering aims to partition the graph into two clusters $S$ and $V-S$, such that 1) $S$ contains the seed node $s$, and 2) the connection between $S$ and $V-S$ is sparse. More precisely, let $vol(S)=\sum_{v \in S}d(v)$ denote the volume of set $S$, and $cut(S)=\{(u,v)\in E \mid u \in S, v \in V-S \}$ denote the set of edges cross $S$ and $V-S$. The quality of the partition is evaluated by the {\bf conductance} of $S$, which is defined as $\Phi(S)=\frac{|cut(S)|}{\min\{vol(S),2m-vol(S)\}}$. Intuitively, a partition $(S, V-S)$ with small conductance tries to minimize the number of edges cross $S$ and $V-S$ and balance the number of edges inside $S$ and $V-S$. 
 


% However, optimizing the conductance $\Phi(S)$ is a NP-hard problem. Hence, comprehensive studies focus on approximate clustering and propose multiple techniques to improve the accuracy and efficiency of clustering. Among them, a line of research~\cite{Teng2004Nibble,FOCS06_FS,chung2007HKPR,chung2018computing,yang2019TEA} focuses on local clustering by {\bf sweeping} through a  proximity vector. More specifically, given a source node $s$, we first computes an approximate proximity (e.g., transition probaility, Personalized PageRank, heat kernal PageRank) vector $\vec{\pi}$ of $s$. Then we sort the nodes $ v_1, \ldots, v_n \in V$ in descending order of $\mathbf{D}^{-1}\cdot \vec{\pi}$  such that $\frac{\vec{\pi}(v_1)}{d_{v_1}} \ge \ldots \ge \frac{\vec{\pi}(v_n)}{d_{v_n}}$. We sweep through the $v_1, \ldots, v_n$ to find the partition with minimum conductance among the subsets $S_i=\{v_1, \ldots, v_i\}, i=1,\ldots, n-1$. Due to the local property of the approximate proximity vector, most of the entries in  $\vec{\pi}$ are zero, and thus we only need to visit a small number of nodes to find a good partition. 

% Along this line of research, 
% Spielman and Teng~\cite{Teng2004Nibble} first introduce the Nibble algorithm that computes the approximate transition probability vector with truncated power iteration and sweeps over the vector to form a good partition. Andersen et al.~\cite{FOCS06_FS} propose PageRank-Nibble, which replaces the transition probability with Personalized PageRank and uses Forward Search to enable efficient approximate Personalized PageRank. Recently, it is shown~\cite{yang2019TEA,chung2018computing} that heat kernel PageRank outperforms transition probability and PageRank to find partitions with small conductance. Finally, we note that our AGP framework can also be used for efficient local clustering due to the ability to simulate transition probability, PPR, and HKPR. 

 

%\subsection{Proximity-based Graph Neural Networks} 
\header{\bf Proximity-based Graph Neural Networks.}
Consider an undirected graph $G=(V,E)$, where $V$ and $E$ represent the set of vertices and edges. Each node $v$ is associated with a numeric feature vector of dimension $d$. The $n$ feature vectors form an $n \times d$ matrix $\mathbf{X}$. Following the convention of graph neural networks~\cite{kipf2016GCN,hamilton2017graphSAGE}, we assume each node in $G$ is also attached with a self-loop.  The goal of graph neural network is to obtain an $n\times d'$ representation matrix $\mathbf{Z}$, which encodes both the graph structural information and the feature matrix $\mathbf{X}$.
%We then feed $\mathbf{Z}$ into a neural network for downstream machine learning tasks such as node classification and link prediction. 
Kipf and Welling~\cite{kipf2016GCN} propose the vanilla Graph Convolutional Network (GCN), of which the $\ell$-th representation $\mathbf{H}^{(\ell)}$ is defined as% $\mathbf{H}^{(\ell)}=\sigma\left(\mathbf{{D}^{-\frac{1}{2}}} \mathbf{{A}}\mathbf{{D}^{-\frac{1}{2}}}\mathbf{H}^{(\ell-1)} \mathbf{W}^{(\ell)}\right)$,
\begin{equation}
\begin{aligned}\label{eqn:definition_GCN}
	\mathbf{H}^{(\ell)}=\sigma\left(\mathbf{{D}^{-\frac{1}{2}}} \cdot \mathbf{{A}} \cdot \mathbf{{D}^{-\frac{1}{2}}} \cdot \mathbf{H}^{(\ell-1)} \cdot \mathbf{W}^{(\ell)}\right),
\end{aligned}	
\end{equation}
where $\mathbf{A}$ and $\mathbf{D}$ are the adjacency matrix and the diagonal degree matrix of $G$, $\mathbf{W}^{(\ell)}$ is the learnable weight matrix, and $\sigma(.)$ is a non-linear activation function (a common choice is the Relu function). Let $L$ denote the number of layers in the GCN model. The $0$-th representation $\mathbf{H}^{(0)}$ is set to the feature matrix $\mathbf{X}$, and the final representation matrix $\mathbf{Z}$ is the $L$-th representation $\mathbf{H}^{(L)}$. Intuitively, GCN aggregates the neighbors' representation vectors from the $(\ell-1)$-th layer to form the representation of the $\ell $-th layer. Such a simple paradigm is proved to be effective in various graph learning tasks~\cite{kipf2016GCN,hamilton2017graphSAGE}. 


% where $\mathbf{\tilde{A}}$ is adjacency matrix of $G$ with added self-connections, $\mathbf{\tilde{D}}$ is the corresponding degree matrix, $\mathbf{W}^{(l)}$ is the learnable weight matrix and $\sigma(.)$ is the activation function.



A major drawback of the vanilla GCN is the lack of ability to scale on graphs with millions of nodes. Such limitation is caused by the fact that the vanilla GCN uses a full-batch training process and stores each node's representation in the GPU memory. 
To extend GNN to large graphs, a line of research focuses on decoupling prediction and propagation, which removes the non-linear activation function $\sigma(.)$ for better scalability. These methods first apply a proximity matrix $\P$ to the feature matrix $\mathbf{X}$ to obtain the representation matrix $\mathbf{Z}$, and then feed $\mathbf{Z}$ into logistic regression or standard neural network for predictions. 
%Among them, SGC~\cite{wu2019SGC} simplifies the vanilla GCN by taking the multiplication of $L$-th power of the normalized adjacency matrix $\mathbf{A}$ and feature matrix  $\mathbf{X}$ to form the final presentation $\mathbf{Z}=\P\cdot \X=\left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^L \cdot \X$.
Among them, SGC~\cite{wu2019SGC} simplifies the vanilla GCN by taking the multiplication of the $L$-th normalized transition probability matrix $\P=\left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^L$ and feature matrix  $\mathbf{X}$ to form the final presentation $\mathbf{Z}=\P\cdot \X=\left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^L \cdot \X$.  %(see Equation~\eqref{eqn:SGC}).
% state-of-art scalable GNNs use various techniques to decouple prediction and propagation and approximate the full neighbor propagation. The main models are: 
% \begin{equation}
% \begin{aligned}\label{eqn:SGC}
% \mathbf{Z}= \left(\mathbf{D}^{-\frac{1}{2}} \cdot \mathbf{A} \cdot \mathbf{D}^{-\frac{1}{2}} \right)^L \cdot \vec{X},
% \end{aligned}	
% \end{equation}
% To make prediction, SGC performs standard logistic regression $\mathbf{y} = SoftMax\left(\mathbf{Z}\cdot \mathbf{W}\right)$, where $\mathbf{W}$ is a trainable weight matrix.  It is easy to see that if we set $a=\frac{1}{2}$, $b=\frac{1}{2}$, $w_i=0 (i =0,\ldots, L-1), w_L=1$ and  $\vec{x}$ to be a column of the feature matrix $\mathbf{X}$, then SGC also falls into the framework of AGP. 
The proximity matrix $\P$ can be generalized to PPR used in APPNP~\cite{Klicpera2018APPNP} and HKPR used in GDC~\cite{klicpera2019GDC}. Note that the original APPNP~\cite{Klicpera2018APPNP} first applies an one-layer neural network to $\mathbf{X}$ before the propagation that $\mathbf{Z}^{(k+1)}=f_\theta(\mathbf{X})$. Then APPNP propagates the feature matrix $\mathbf{X}$ with a truncated Personalized PageRank matrix $\mathbf{Z} =\sum_{i=0}^L \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^i \cdot f_\theta(\mathbf{X})$, where $\alpha$ is a constant in $(0,1)$. The original GDC~\cite{klicpera2019GDC} follows the structure of GCN and substitutes the diffusion kernel to heat kernel. 
For the sake of high scalability, unless specified otherwise, we use APPNP and GDC to denote the linear propagation process that $\mathbf{Z} =\sigma\left(\sum_{i=0}^L \P^{(i)} \cdot \X\right)$ throughout the paper, where $L$ is the number of layers. Specifically, $\P^{(i)}=\alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^i$ for APPNP and $\P^{(i)}=\frac{e^{-t} t^i}{i!}\cdot \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^i$ for GDC. 
%APPNP~\cite{Klicpera2018APPNP} propagates the feature matrix $\mathbf{X}$ with a truncated Personalized PageRank matrix $\mathbf{Z} =\sum_{i=0}^L \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^i \cdot \X,$
%% \begin{equation}
%% \begin{aligned}\label{eqn:APPNP}
%% \mathbf{Z} =\sum_{i=0}^L \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-\frac{1}{2}} \cdot \mathbf{A} \cdot \mathbf{D}^{-\frac{1}{2}} \right)^i \cdot \vec{X},
%% \end{aligned}	
%% \end{equation}
%where $\alpha$ is a constant in $(0,1)$ and $L$ is the number of layers. If the feature dimension $d$ is too large, APPNP may also apply an one-layer neural network to $\mathbf{X}$ before the propagation to reduce the dimension. 
% Similarly, the AGP framework is able to express APPNP by setting $a=\frac{1}{2}$, $b=\frac{1}{2}$, $w_i=\alpha(1-\alpha)^i$, and  $\vec{x}$ to be a column of the feature matrix $\mathbf{X}$. 
%GDC~\cite{klicpera2019GDC} uses heat kernel PageRank to propagate the features: $\mathbf{Z} = \sum_{i=0}^L \frac{e^{-t} t^i}{i!}\cdot \left(\mathbf{D}^{-\frac{1}{2}}\mathbf{A} \mathbf{D}^{-\frac{1}{2}} \right)^i \cdot \X.$
%% \begin{equation}
%% \begin{aligned}\label{eqn:GDC}
%% \mathbf{Z} = \sum_{i=0}^L e^{-t} \cdot \frac{t^i}{i!}\cdot \left(\mathbf{D}^{-\frac{1}{2}} \cdot \mathbf{A} \cdot \mathbf{D}^{-\frac{1}{2}} \right)^i \cdot \vec{X}.
%% \end{aligned}	
%% \end{equation}
%% Similarly, the GDC framework is able to express GDC by setting $a=\frac{1}{2}$, $b=\frac{1}{2}$, $w_i = e^{-t} \cdot \frac{t^i}{i!}$, and  $\vec{x}$ to be a column of the feature matrix $\mathbf{X}$. 

% We also note that our AGP framework is suitable for graph neural networks, as the downstream learning tasks can usually tolerate a small error in order to achieve faster training time and higher scalability.


A recent work PPRGo~\cite{bojchevski2020scaling} improves the scalability of APPNP by employing the Forward Search algorithm~\cite{FOCS06_FS} to perform the propagation. However, PPRGo only works for APPNP, which, as we shall see in our experiment, may not always achieve the best performance among the three models.  Finally,
a recent work GBP~\cite{chen2020GBP} proposes to use deterministic local push and the Monte Carlo method to approximate GNN propagation of the form $\Z=\sum_{i=0}^L w_i \cdot \left(\mathbf{D}^{-(1-r)}\mathbf{A} \mathbf{D}^{-r} \right)^i \cdot \X.$ However, GBP suffers from two drawbacks: 1) it requires $a+b = 1$ in Equation~\eqref{eqn:pi_gen} to utilize the Monte-Carlo method, and 
%2) it may incur unbounded error in the context of proximity queries (i.e. Definition~\ref{def:pro-relative}). In particular, consider a graph propagation process  with the reverse transition matrix $\D^{-1}\A$. GBP will amplify the error by a factor of $d_u$ for the estimator $\epi(u)$ of any node $u\in V$. The detailed illustrations can be found in the technical report~\cite{TechnicalReport}.
2) it requires $O(n)$ space cost in the Monte-Carlo process, which limits its scalability on large graphs with billions of edges. As we shall see in Section~\ref{sec:exp}, to run GBP on Friendster and Papers100M, we have to reduce the forward Monte-Carlo phase and only conduct the reverse push to avoid the out-of-memory problems. 


% there are methods~\cite{zeng2019graphsaint,zou2019layer,chen2018fastgcn}that utilize sampling techniques to reduce the time complexity inside each layer of GCN. These methods are orthogonal to the techniques discussed in this paper. 





% to find cuts by PageRank vectors derived by Forward Search.  

% to generate random walks from the seed node, and use the distribution of transition probabilities to find small cuts. 

% Recently, the state-of-the-art works compute heat kernel PageRank to find clusters. 
%Given a seed node $s$ on the graph $G=(V,E)$, these work first compute the normalized heat kernel PageRank $\frac{\rho(s,u)}{d_u}$ for each node $u\in V$. Then they conduct a sweep operation based on sorted HKPR values to find clustering results. In the process, 



%\header{\bf Heat Kernel PageRank. }

% Weting  can random waalso visimulaew PPR scores from the probability aspect. The PPR value $\pi(s,t)$ equals the probability that the $\alpha$-discounted random walk starting from node $s$ terminates at node $t$ in the end. Hence, the sum of single-source PPR equals 1 that $\sum_{t\in V}\pi(s,t)=1$. Here, single-source PPR is a kind of PPR query, which asks for the PPR values of every node to a given source node $s$. 




% where $\pi_s$ denotes the single-source PPR vector, $\vec{e}_s$ is the one-hot vector with $\vec{e}_s(s)=1$. Let $\mathbf{P}^\top=\mathbf{A}^\top \cdot \mathbf{D}^{-1}$, which represents the reverse transition probability and $\mathbf{P}^\top(i,j)=\frac{1}{d_{out}(j)}$ if $(v_j,v_i)\in E$. 
% By equation~\eqref{eqn:ss_PPR_definition}, we can update vector $\pi_s$ iteratively, and finally derive the power equation for single-source PPR that: 

%By equation~\eqref{eqn:power_ssPPR}, each iteration costs $O(m)$ to update PPR values. After $L=\log_{1-\alpha}{\delta}$ iterations, the power method can bound the PPR results with relative error threshold $\delta$. Thus, the query complexity of the power method is $O\left( m\cdot \log_{1-\alpha}{\delta}\right)$. The power method can derive high precision PPR results because the complexity is logarithmic of error threshold $\delta$. But when $m$ becomes large on large-scale graph, it is inefficient for approximation computation. 
%In each iteration, power method needs to cost $O(m)$ to update PPR values for neighbors. 
%Hence, power method can derive high precision PPR results because its query complexity . However, in each iteration, power method needs to update the neighbors of every node with nonzero PPR scores. By the small-world theorey, the number of nodes with nonzero PPR can quickly comes to $O(m)$, where $m$ is the number of edges on the graph. For large graphs, this can be quite inefficient for approximation.  

%It set residue $r^f(s,u)$ to denote the probability mass that is distributed to each node $u \in V$, and reserve $\pi^f(s,u)$ to represent the probability terminating at node $u$. The residue of source node $s$ $r^f(s,s)$ is initialized as 1. And in each forward push, the node $u$ with residue $r^f(s,u)>\delta$ will convert $\alpha$ percentage to its reserve $\pi^f(s,u)$, and transfer the other $(1-\alpha)\cdot r^f(s,u)$ to the residues of its out-neighbors. After the process, the reserve of each node can be regarded as the estimation of PPR. Beyond that, forward search and Monte-Carlo Sampling can be combined together to further improve the query efficiency~\cite{FORA_17KDD,wei2018topppr}.





%Similarly, given a target node $t$, Backward Search sets residue $r^b(u,t)$ and reserve $\pi^b(u,t)$ for each node $u \in V$. In each backward push step, the node who satisfies $r^b(u,t)>\delta$ leaves $\alpha$ fraction to its reserve, and transfer $\frac{(1-\alpha)\cdot r^b(u,t)}{d_{out}(v)}$ to every in-neighbors $v\in N_{in}(u)$. 
%RBS~\cite{wang2020RBS}


%Moreover, PPR has been intensively applied in many applications, such as heavy hitters query, SimRank computation, graph matching, graph neural network and so on. 




%Studies on PPR computation can be classified into four categories: 1) single-pair PPR, which asks the PPR values of one pair of nodes; 2) single-source PPR, which asks the PPR values of each nodes to a given source node; 3) single-target PPR, which asks the PPR values of a given target node to every node on the graph; 4) all-pairs PPR, which asks the PPR value of every pair of nodes. 
%Because single-pair PPR value can be found as an entry of single-source/target PPR vector, and all-pairs PPR query can be derived by conducting single-source/target PPR for each node as source/target node. Hence, PPR studies mainly focus on the effective computation for single-source/target PPR query. The 


%From the probability aspect, Hence, $\frac{1}{n} \cdot \sum_{s\in V}\pi(s,t)=n\cdot \pi(t)$. 



%  Monte-Carlo sampling is infeasible for single-target PPR query, as the random walks can not be generated reversely from the target node; Instead, Backward Search~\cite{lofgren2013personalized} and Randomized Backward Search are used for computing the approximate single-target PPR vector. 
 
 

 

% \header{\bf PageRank~\cite{page1999pagerank} } is developed by Google to rank web pages on the world wide web. 
% The intuition of PageRank is  "a page is important if they are referenced by many pages, or important pages". 
% More specifically, given an undirected graph $G=(V,E)$ with $n$ nodes and $m$ edges, PageRank can be formalized as a solution to the following equation:
% \begin{equation}\label{eqn:PageRank_definition}
% 	\begin{aligned}
% 		\vec{\pi}=\left( 1-\alpha \right) \cdot \mathbf{A}\mathbf{D}^{-1} \cdot \vec{\pi}+ \alpha\cdot \left(\frac{1}{n}\cdot \vec{1} \right), 
% 	\end{aligned}
% \end{equation}
% where $\vec{\pi}$ denotes the PageRank vector, $\vec{1}$ is the vector with $1$ for each entry, $\alpha\in [0,1]$ is the teleport probability. Intuitively, the PageRank vector $\vec{\pi}$ is the stationary probability distribution of the {\bf random walk with teleport}. The random walk starts with the uniform distribution $\frac{1}{n}\cdot \vec{1}$ and, at each step, move to random neighbor of the current node with probability $1- \alpha$, or teleports to a random node with probability $\alpha$. 
% Denote {\it $\alpha$-discounted random walk} as a random traversal that at each step, the walk terminates at the current step with the probability $\alpha$, or moves to a randomly selected out-neighbor with the probability $1-\alpha$. We can understand PageRank as a probability scores of random walk process. 
% That is, generate an $\alpha$-discounted random walk from a randomly selected node on the graph. The PageRank score $\pi(t)$ equals the probability that the $\alpha$-discounted random walk terminates at node $t$ eventually. 
%Note that the nodes with more in-edges are more easily arrived by other nodes, and then have higher PageRank scores. 
%Intuitively, the PageRank vector is the stationary distribution of  evaluates the overall importance of nodes on the graph. 

% The unique solution to the PageRank equation~\eqref{eqn:PageRank_definition} is given by the following formula:
% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\vec{\pi}=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A} \mathbf{D}^{-1} \right)^i \cdot \left(\frac{1}{n}\cdot \vec{1} \right),
% 	\end{aligned}
% \end{equation}
%  which can be expressed by the graph propagation if we set  $a=0, b=1, w_i=\alpha \cdot \left(1-\alpha\right)^i$, and $\vec{x}=\frac{1}{n}\cdot \vec{1}$ in equation~\eqref{eqn:pi_gen}. PageRank reflects the structural importance of each node, and thus is widely used in graph mining tasks~\cite{influence, webmining}. 
 

%  \header{\bf  Hyperlink-Induced Topic Search (HITS)}~\cite{kleinberg1999HITS} is proposed as an alternative to PageRank for rating the web pages. It divides all web pages into two categories, hubs and authorities. Hub pages are served as link collections to direct surfers towards authority pages. 
% HITS considers a mutually reinforcing relationship between hub pages and authority pages: good hub pages contain many high-quality links to good authority pages, while good authorities should be pointed by multiple good hubs.  
% This intuition can be summarized as two types of iterative equation that
% \begin{equation}\nonumber %\label{eqn:definition-HITS}
% 	\begin{aligned}
% 	\pi_{hub}=\mathbf{A}\cdot \pi_{auth}=\mathbf{A}\mathbf{A}^\top \cdot \pi_{hub};\\
% 	\pi_{auth}=\mathbf{A}^\top \cdot \pi_{hub}=\mathbf{A}^\top \mathbf{A}\cdot \pi_{auth}, 
% \vspace{-1mm}
% \end{aligned}
% \end{equation}
% where $\pi_{hub}$ and $\pi_{auth}$ denote the HITS vectors derived by the hub/authority update rule. For  initialization, HITS sets the rank score of each node on the graph as $\left(\frac{1}{n}\cdot\vec{1}\right)$, no matter hub or authority. After every iteration,  HITS normalizes the two vectors to guarantee the sum of each entry equals 1. For  undirected graphs, we can unify the two update rules as below~\cite{}. %given in equation~\eqref{eqn:definition-HITS} 
% \begin{align}\nonumber
% 	\vec{\pi}\sum_{j=0}^\infty \frac{1}{\mu_j}\cdot \left(\mathbf{A}^\top\right)^{2j} \cdot \left(\frac{1}{n}\cdot\vec{1}\right). 
% \end{align}
% $\mu_j$ denotes the sum of each entry of the HITS vector after $j_{th}$ iteration, which can normalize the HITS scores to assure convergence. 
% Let $a=b=0, \vec{x}=\vec{1}$, $w_i=\frac{1}{\mu_j}$ when $i=2j$ and $w_i=0$ if $i$ is an odd number. Our generalized propagation equation can be changed to HITS computational equation. 




% A large number of works emerge to measure the node proximity more scientifically and effectively, such as PageRank and Personalized PageRank~\cite{page1999pagerank}, Katz~\cite{katz1953Katz}, HITS~\cite{kleinberg1999HITS} and so on. 
% These methods express the node proximity based on different intuitions. % and are applied to their target application scenarios respectively.  
% We will first give a brief introduction towards these metrics, then explain how to unify them by our generalized propagation equation in detail.  






% \header{\bf Personalized PageRank (PPR)~\cite{page1999pagerank} } is a variant of PageRank that aims to evaluate the relative significance of a target node with regards to a source node. In particular, the (single-source) PPR vector of a source node $s$ is given by the solution of the following equation: 
% \begin{equation}\label{eqn:ss_PPR_definition}
% 	\begin{aligned}
% 		\pi_s=\left( 1-\alpha \right) \cdot \left(\mathbf{A} \mathbf{D}^{-1} \right) \cdot \pi_s+  \alpha \cdot \vec{e}_s, 
% 	\end{aligned}
% \end{equation}
% Compared to the PageRank equation~\eqref{eqn:PageRank_definition},  PPR replace the starting distribution $\frac{1}{n}\cdot \vec{1}$ with an one-hot vector $\vec{e}_s$, which means the random walk will teleport back to the source node $s$ at each step with probability $\alpha$. The PPR vector $\vec{\pi}$ measures the importance of each node $t$ with respect to $s$. The unique solution to Equation~\eqref{eqn:ss_PPR_definition} is given by the following formula:
% \begin{align}\label{eqn:power_ssPPR}
% 	\pi_s=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{A}  \cdot \mathbf{D}^{-1} \right)^i \cdot \vec{e}_s, 
% \end{align}
% which concurs with our generalized propagation equation~\eqref{eqn:pi_gen} by setting $a=0, b=1, w_i=\alpha\left( 1-\alpha \right)^i, \vec{x}=\vec{e}_s$. Single-source PPR can be estimated by Monte-Carlo sampling~\cite{Fogaras2005MC}, which generates adequate random walks from the source node $s$ and uses the percentage of the walks that terminate at node $t$ as the estimation of $\vec{\pi}_s(t)$. Moreover, Forward Search~\cite{FOCS06_FS} conducts local push from the source node $s$ to estimate PPR scores step by step. FORA~\cite{FORA_17KDD} combines Forward Search with Monte Carlo to improve the query efficiency. TopPPR combines Forward Search, Monte Carlo and Backward Search~\cite{lofgren2013personalized} to obtain a better error guarantee for estimating top-$k$ PPR scores.


% Anther line of research~\cite{} studies the {\bf single-target PPR}, which asks for the PPR value of every node to a given target node $t$ on the graph.  The single-target PPR vector for a given node $\vec{\pi(t)}$ is defined by the following equation:
% \begin{equation}\label{eqn:st_PPR_definition}
% 	\begin{aligned}
% 		\pi_t=\left( 1-\alpha \right) \cdot \left(\mathbf{D}^{-1} \mathbf{A} \right)\cdot \pi_t+\alpha \cdot \vec{e}_t, 
% 	\end{aligned}
% \end{equation}
% Unlike the single-source PPR vector, the single-target PPR vector $\vec{\pi}_t$ is not a probability distribution, which means $\sum_{s\in V}\vec{\pi}_t(s)$ may not equal to $1$. 
% %  $\vec{e}_t$ is the one-hot vector that $\vec{e}_t(t)=1$. And $\mathbf{P}=\mathbf{D}^{-1} \mathbf{A}$ is the transition probability matrix that $\mathbf{P}(i,j)=\frac{1}{d_{out}(i)}$ if $(v_i,v_j)\in E$. 
% We can also derive the unique solution to Equation~\eqref{eqn:st_PPR_definition} by
% \begin{equation}\nonumber
% 	\begin{aligned}
% 		\pi_t=\sum_{i=0}^\infty \alpha \left( 1-\alpha \right)^i \cdot \left(\mathbf{D}^{-1} \cdot \mathbf{A} \right)^i \cdot \vec{e}_t, 
% 	\end{aligned}
% \end{equation}
%  which can be formulated as a AGP process~\eqref{eqn:pi_gen} by setting $a=1,b=0, w_i=\alpha \left( 1-\alpha \right)^i, \vec{x}=\vec{e}_t$. Monte-Carlo sampling is infeasible for single-target PPR query, as the random walks can not be generated reversely from the target node; Instead, Backward Search~\cite{lofgren2013personalized} and Randomized Backward Search are used for computing the approximate single-target PPR vector.  
% Denote {\it $\alpha$-discounted random walk} as a random traversal that at each step, the walk terminates at the current step with the probability $\alpha$, or moves to a randomly selected out-neighbor with the probability $1-\alpha$. We can understand PageRank as a probability scores of random walk process. 
% That is, generate an $\alpha$-discounted random walk from a randomly selected node on the graph. The PageRank score $\pi(t)$ equals the probability that the $\alpha$-discounted random walk terminates at node $t$ eventually. 
%Note that the nodes with more in-edges are more easily arrived by other nodes, and then have higher PageRank scores. 
%Intuitively, the PageRank vector is the stationary distribution of  evaluates the overall importance of nodes on the graph. 

% We note that the above models can be simulated and by equation~\eqref{eqn:pi_gen}. By setting $a=b=\frac{1}{2}$ , if we set $w_i=0 (i \neq k), w_k=1$, equation~\eqref{eqn:pi_gen} simulates the $k$-th transition probability matrix in GCN and SGC. One the other hand, if we set $w_i=\alpha \left( 1-\alpha \right)^i$ for some constant decay factor $\alpha\in(0,1)$, the equation becomes Personalized PageRank used in APPNP and PPRGo. Finally, if we set $w_i=e^{-t} \cdot \frac{t^i}{i!} $, the equation simulates the Heat Kernal PageRank with diffusion time $t$ in GDC.



%\header{\bf Forward Search.} 
%~\cite{AndersenCL06}


%\header{\bf Backward Search.} 
%~\cite{lofgren2013personalized}




%%% Local Variables:
%%% mode: latex
%%% TeX-master: "paper"
%%% End:

