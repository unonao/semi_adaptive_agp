%\clearpage
\begin{table*}[t]
	%\vspace{-1mm}
	%\centering
	%\lefting
	\tblcapup
	\caption{Hyper-parameters of AGP.}
	\vspace{-5mm}
	\tblcapdown
	\begin{small}
	\begin{threeparttable}
		\begin{tabular}{|c|c|c|c|c|c|c|c|c|c|c|}
			\hline
			\multirow{2}{*}{{\bf Data set}} & \multirow{2}{*}{\bf Learning rate}& \multirow{2}{*}{{\bf Dropout}} & {\bf Hidden} &\multirow{2}{*}{\bf Batch size} & {\bf GDC-AGP}&{\bf APPNP-AGP}&\bf SGC-AGP &\multicolumn{3}{c|}{\bf AGP} \\ \cline{6-11}
			~&&&{\bf dimension} & &$\boldsymbol{t}$& $\boldsymbol{\alpha}$& $\boldsymbol{L}$& {\bf $a$} &{\bf $b$}& {\bf $w_i$} \\ \hline
Yelp & 0.01 & 0.1 & 2048 & $3\cdot 10^4$&  4 & 0.9& 10 & - & - & -\\
Amazon  & 0.01 & 0.1 & 1024 &$10^5$& 4 & 0.2& 10 & 0.8 & 0.2 & $\alpha(1-\alpha)^i, \alpha=0.2$\\
Reddit  & 0.0001 & 0.3 &  2048  &$10^4$& 3 & 0.1& 10 & - & -& -\\
Papers100M & 0.0001 & 0.3 & 2048 &$10^4$& 4 & 0.2& 10 & 0.5 & 0.5 & $e^{-t}\cdot \frac{t^i}{i!}, t=4$\\ 
			\hline
		\end{tabular}
	\end{threeparttable}
	\end{small}
	\label{tbl:parameters}
	%\tbldown
	\vspace{-1mm}
\end{table*}

\section{Additional Experimental Descriptions}\label{sec:appendix}

\subsection{Local clustering with HKPR}

\header{\bf Methods and Parameters. }
For our method, we set $a=0,b=1, w_i=\frac{e^{-t}t^i}{i!}$, and $\vec{x}=\vec{e}_s$ in Equation~\eqref{eqn:pi_gen} to simulate the HKPR equation $\vec{\pi}=\sum_{i=0}^\infty \frac{e^{-t}t^i}{i!}\cdot \left(\mathbf{A}\mathbf{D}^{-1}\right)^i\cdot \vec{e}_s$. We employ the randomized propagation algorithm~\ref{alg:AGP-RQ} with level number $L= O\left(\log {1/\delta}\right)$ and error parameter $\varepsilon = \frac{2\delta}{L(L+1)}$, where $\delta$ is the relative error threshold in Definition~\ref{def:pro-relative}. We use AGP to denote this method. We vary $\delta$ from $0.1$ to $10^{-8}$ to obtain a trade-off curve between the approximation quality and the query time. 
%We also include the basic propagation algorithm~\ref{alg:AGP-deter} with $L = 50$ (denoted as Basic), which serves as the algorithm for computing ground and as a baseline for evaluating the trade-off curves of the approximate algorithms.
We use the results derived by the Basic Propagation Algorithm~\ref{alg:AGP-deter} with $L = 50$ as the ground truths for evaluating the trade-off curves of the approximate algorithms.

We compare AGP with four local clustering methods: TEA~\cite{yang2019TEA} and its optimized version TEA+, ClusterHKPR~\cite{chung2018computing}, and the Monte-Carlo method (MC). 
TEA~\cite{yang2019TEA}, as the state-of-the-art clustering method, combines a deterministic push process with the Monte-Carlo random walk. Given a graph $G=(V, E)$, and a seed node $s$, TEA conducts a local search algorithm to explore the graph around $s$ deterministically, and then generates random walks from nodes with residues exceeding a threshold parameter $r_{max}$.  One can manipulate $r_{max}$ to balance the two processes. It is shown in~\cite{yang2019TEA} that TEA can achieve $O\left(\frac{t\cdot \log{n}}{\delta}\right)$ time complexity, where $t$ is the constant heat kernel parameter.
ClusterHKPR~\cite{chung2018computing} is a Monte-Carlo based method that simulates adequate random walks from the given seed node and uses the percentage of random walks terminating at node $v$ as the estimation of $\vec{\pi}(v)$. 
%In the $k_{th}$ step, the walk stops at the node it currently walks at with the probability , or move to a randomly selected neighbor with the probability $\frac{t}{k}$. 
The length of walks $k$ follows the Poisson distribution $\frac{e^{-t}t^k}{k!}$. The number of random walks need to achieve a relative error of $\delta$ in Definition~\ref{def:pro-relative} is $O\left(\frac{t\cdot \log{n}}{\delta^3}\right)$. 
MC~\cite{yang2019TEA} is an optimized version of random walk process that sets identical length for each walk as $L=t\cdot \frac{\log{1/\delta}}{\log{\log{1/\delta}}}$. If a random walk visit node $v$ at the $k$-th step,  we add $\frac{e^{-t}t^k}{n_r \cdot k!}$ to the propagation results $\vec{\epi}(v)$, where $n_r$ denotes the total number of random walks. The number of random walks to achieve a relative error of $\delta$ is also $O\left(\frac{t\cdot \log{n}}{\delta^3}\right)$. Similar to AGP, for each method, we vary $\delta$ from $0.1$ to $10^{-8}$ to obtain a trade-off curve between the approximation quality and the query time. 
Unless specified otherwise, we set the heat kernel parameter $t$ as 5, following~\cite{kloster2014heat, yang2019TEA}. All local clustering experiments are conducted on a machine with an Intel(R) Xeon(R) Gold 6126@2.60GHz CPU and 500GB memory. 


\subsection{Node classification with GNN}

\header{\bf Datasets. }
Following~\cite{zeng2019graphsaint,zou2019layer}, we perform inductive node classification on Yelp, Amazon and Reddit, and semi-supervised transductive node classification on Papers100M. More specifically, for inductive node classification tasks, we train the model on a graph with labeled nodes and predict nodes' labels on a testing graph. For semi-supervised transductive node classification tasks, we train the model with a small subset of labeled nodes and predict other nodes' labels in the same graph. We follow the same training/validation/test-ing split as previous works in GNN~\cite{zeng2019graphsaint,hu2020ogb}. 





\header{\bf GNN models.} 
We first consider three proximity-based GNN models: APPNP~\cite{Klicpera2018APPNP},SGC~\cite{wu2019SGC}, and GDC~\cite{klicpera2019GDC}. We augment the three models with the AGP Algorithm~\ref{alg:AGP-RQ} to obtain three variants: APPNP-AGP, SGC-AGP and GDC-AGP. Take SGC-AGP as an example. Recall that SGC uses $\mathbf{Z}=\left(\mathbf{D}^{-\frac{1}{2}} \mathbf{A}\mathbf{D}^{-\frac{1}{2}} \right)^L \hspace{-1mm}\cdot \X$ to perform feature aggregation, where $\X$ is the $n\times d$ feature matrix. SGC-AGP treats each column of $\X$ as a graph signal $\bm{x}$ and perform randomized propagation algorithm (Algorithm~\ref{alg:AGP-RQ}) with predetermined error parameter $\delta$ to obtain the the final representation $\mathbf{Z}$. To achieve high parallelism, we perform propagation for multiple columns of $\mathbf{X}$ in parallel. Since APPNP and GDC's original implementation cannot scale on billion-edge graph Papers100M, we implement APPNP and GDC in the AGP framework. In particular, we set $\varepsilon = 0$ in Algorithm~\ref{alg:AGP-RQ} to obtain the exact propagation matrix $\mathbf{Z}$, in which case the approximate models APPNP-AGP and GDC-AGP essentially become the exact models APPNP and GDC. We set $L\hspace{-1mm}=\hspace{-1mm}20$ for GDC-AGP and APPNP-AGP, and $L\hspace{-1mm}=\hspace{-1mm}10$ for SGC-AGP. Note that SGC suffers from the over-smoothing problem when the number of layers $L$ is large~\cite{wu2019SGC}. We vary the parameter $\varepsilon$ to obtain a trade-off curve between the classification accuracy and the computation time. 

\begin{table}[t]
	%\vspace{-2mm}
	%\centering
	%\lefting
	\tblcapup
	\caption{URLs of baseline codes.}
	\vspace{-3mm}
	\tblcapdown
	\begin{small}
		%\begin{tabular}{|c|c|c|} %p{1.3in}|}
			
		\begin{tabular}{|c|c|}\hline
	%{\bf Methods} & {\bf URL} & \hspace{-2mm}{\bf Commit}\hspace{-5mm}\\ \hline
    %GDC & https://github.com/klicperajo/gdc & \hspace{-2mm}14333fd\hspace{-2mm}\\
    %APPNP & https://github.com/rusty1s/pytorch$\_$geometric & \hspace{-2mm}f560655\hspace{-2mm}\\
    %SGC & https://github.com/Tiiiger/SGC &\hspace{-2mm}795ec93 \hspace{-2mm} \\ 
    %PPRGo & https://github.com/TUM-DAML/pprgo$\_$pytorch &\hspace{-2mm} d9f991e\hspace{-2mm}\\ 
    %GBP & https://github.com/chennnM/GBP &\hspace{-2mm}f811fc2 \\
    %\hspace{-2mm}ClusterGCN\hspace{-2mm} &\hspace{-2mm}https://github.com/benedekrozemberczki/ClusterGCN \hspace{-2mm} &\hspace{-2mm}a6b40cc\hspace{-2mm} \\ \hline
    {\bf Methods} & {\bf URL} \\ \hline
    GDC & https://github.com/klicperajo/gdc \\
    APPNP & https://github.com/rusty1s/pytorch$\_$geometric \\
    SGC & https://github.com/Tiiiger/SGC \\ 
    PPRGo & https://github.com/TUM-DAML/pprgo$\_$pytorch \\ 
    GBP & https://github.com/chennnM/GBP \\ 
    ClusterGCN &https://github.com/benedekrozemberczki/ClusterGCN \\ \hline
	\end{tabular}
	\end{small}
	\label{tbl:url}
	%\tbldown
	%\vspace{-2mm}
\end{table}


Besides, we also compare AGP with three scalable methods: PPRGo~\cite{bojchevski2020scaling}, GBP~\cite{chen2020GBP}, and ClusterGCN~\cite{chiang2019clusterGCN}. Recall that PPRGo is an improvement work of APPNP. It has three main parameters: the number of non-zero PPR values for each training node $k$, the number of hops $L$, and the residue threshold $r_{max}$. We vary the three parameters $(k,L,r_{max})$ from $(32,2,0.1)$ to $(64,10,10^{-5})$. GBP decouples the feature propagation and prediction to achieve high scalability. In the propagation process, GBP has two parameters: the propagation threshold $r_{max}$ and the level $L$. We vary $r_{max}$ from $10^{-4}$ to $10^{-10}$, and set $L=4$ following~\cite{chen2020GBP}. ClusterGCN uses graph sampling method to partition graphs into small parts, and performs the feature propagation on one randomly picked sub-graph in each mini-batch. We vary the partition numbers from $10^4$ to $10^5$, and the propagation layers from $2$ to $4$. For AGP, we vary $\delta$ from $10^{-5}$ to $10^{-10}$, and tune $a,b,w_i$ for the best performance. 

For each method, we apply a neural network with 4 hidden layers, trained with mini-batch SGD. %The batch size is set to be $10000$ for faster training time and better generalization ability.
We employ initial residual connection~\cite{He2016ResNet} across the hidden layers to facilitate training. We use the trained model to predict each testing node's labels and take the mean accuracy after five runs. For GDC, APPNP, and SGC, we divide the computation time into two parts: the {\em preprocessing time} for computing $\mathbf{Z}$, and the {\em training time} for performing mini-batch SGD on $\mathbf{Z}$ until convergence. All the experiments in this section are conducted on a machine with an NVIDIA RTX8000 GPU (48GB memory), Intel Xeon CPU (2.20 GHz) with 40 cores, and 512 GB of RAM. Detailed parameter settings can be founded in the appendix. 


\header{\bf Detailed setups. }
Table~\ref{tbl:parameters} summarize the hyper-parameters of AGP. 
Table~\ref{tbl:url} summarizes the available URLs of methods we used. %Note that for GDC and APPNP, we set $r_{max}=0$ to obtain the exact propagation results, because the original implementation of GDC and APPNP don't support the billion-edge graphs.

