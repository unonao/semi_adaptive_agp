

通常：(頂点数、agp特徴量次元)
semi-adaptive版：（頂点数、agp数、agp特徴量次元）


#  Approximate Graph Propagation

Codes Contributors: Hanzhi Wang, Mingguo He
<br/>

## Citation
Please cite our paper when using the codes:

```
@inproceedings{10.1145/3447548.3467243,
author = {Wang, Hanzhi and He, Mingguo and Wei, Zhewei and Wang, Sibo and Yuan, Ye and Du, Xiaoyong and Wen, Ji-Rong},
title = {Approximate Graph Propagation},
year = {2021},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
booktitle = {Proceedings of the 27th ACM SIGKDD Conference on Knowledge Discovery & Data Mining},
pages = {1686–1696},
location = {Virtual Event, Singapore},
series = {KDD '21}
}
```

<br/>

## I. Local Clustering with HKPR
### Tested Environment:
- Ubuntu 16.04.10
- C++ 11
- GCC 5.4.0


### Data:
* All of the datasets used in the paper are publicly available at:
    * YouTube: https://snap.stanford.edu/data/bigdata/communities/com-youtube.ungraph.txt.gz
    * Orkut: https://snap.stanford.edu/data/bigdata/communities/com-orkut.ungraph.txt.gz
    * Friendster: https://snap.stanford.edu/data/bigdata/communities/com-friendster.ungraph.txt.gz
    * Twitter: http://data.law.di.unimi.it/webdata/twitter-2010/twitter-2010.graph

    Additionally, we provide a toy dataset at *clustering-HKPR/dataset/*.

* Each dataset has a unique name called filelabel (e.g. *youtube*, *orkut*, *friendster* and *twitter*) .
* Please rename the raw datasets as *${filelabel}.txt* (e.g. *youtube.txt*, *orkut.txt*, *friendster.txt*, and *twitter.txt*) and put them in the directory: *clustering-HKPR/dataset/*.
* We assume that all raw datasets follow a consistent format:
    * The number of nodes is explicitly specified in the first line of the data file.
    * Each line following the second line indicates a **directed** edge in the graph.
* We assume that all undirected graphs have been converted to directed graphs that each undirected edge appears twice in the data file.
* We assume that the node index starts from $0$. The number of nodes is larger than the largest node index.
* The code converts the raw data to binary file in CSR format when reading raw graphs for the first time. The converted binary data is stored in the directory: *clustering-HKPR/dataset/${filelabel}/*.


### Query nodes:
* When the code is invoked for the first time, it will automatically construct a query file containing 100 query nodes.
* We name the query file as ${filelabel}.query and put it in the directory: *clustering-HKPR/query/*.


### Execution:
We include the fundamental commands in the script file: *clustering-HKPR/run_script.sh*. To automatically execute our codes, please use the following bash commands:
```
bash run_script.sh
```

Alternatively, our codes can be executed mannually. Specifically, to compile the codes:
```
cd clustering-HKPR
rm HKPR
make
```
To run powermethod:
```
./HKPR -d ./ -f youtube -algo powermethod -qn 10 -t 5
```
To run AGP:
```
./HKPR -d ./ -f youtube -algo AGP -e 1e-07 -qn 10 -t 5
```

### Parameters:
- -d \<path of the "AGP-master" directory\> ("./" by default)
- -f \<filelabel\> ("youtube" by default)
- -algo \<algorithm\> ("AGP" by default)
- -e \<epsilon\> (0.001 by default)
- -qn \<querynum\> (10 by default)
- -t \<the heat kernel parameter\> (5 by default)


### Remark:
* *clustering-HKPR/datatset/*: containing the datasets
* *clustering-HKPR/query/*: containing the query files
* *clustering-HKPR/result/*: containing the approximation results.

<br/>

## II. Node Classficiation with GNN
### Requirements
- CUDA  10.1 → 11.1
- python 3.8.5
- pytorch 1.7.1
- GCC 5.4.0
- cython 0.29.21
- eigency 1.77
- numpy 1.18.1
- torch-geometric 1.6.3
- tqdm 4.56.0
- ogb 1.2.4
- [eigen 3.3.9] (https://gitlab.com/libeigen/eigen.git)

### Data
All of the datasets used in the paper are publicly available at:
* Reddit: https://github.com/GraphSAINT/GraphSAINT
* Yelp: https://github.com/GraphSAINT/GraphSAINT
* Amazon2M: https://github.com/google-research/google-research/tree/master/cluster_gcn
* Papers100M: https://ogb.stanford.edu



### Compilation
To compile the code, please run cython first, following  the commands shown as below.
```
python setup.py build_ext --inplace
```


### Execution
* On Reddit dataset:
```
sh reddit.sh
```
* On Yelp dataset:
```
sh yelp.sh
```
* On Amazon2M dataset:
```
sh amazon2M.sh
```
* On Papers100M dataset:
```
sh papers100M.sh
```
